{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO tutorial: Coding YOLO v3\n",
    "https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/\n",
    "\n",
    "This documents describes the code that I wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import darknet\n",
    "import util\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import importlib     # During dev only\n",
    "importlib.reload(darknet)  # During dev only\n",
    "importlib.reload(util) # During dev only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the 'official' config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function `darknet.parse_cfg`.\n",
    "\n",
    "The idea here is to parse the cfg, and store every block as a dict. The attributes of the blocks and their values are stored as key-value pairs in the dictionary. As we parse through the cfg, we keep appending these dicts, denoted by the variable block in our code, to a list blocks. Our function will return this block.\n",
    "\n",
    "We begin by saving the content of the cfg file in a list of strings. The following code performs some preprocessing on this list.\n",
    "\n",
    "    file = open(cfgfile, 'r')\n",
    "    lines = file.read().split('\\n')                        # store the lines in a list\n",
    "    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n",
    "    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n",
    "    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n",
    "    \n",
    "Now loop over the resultant to get blocks\n",
    "\n",
    "    block = {}\n",
    "    blocks = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line[0] == \"[\":               # This marks the start of a new block\n",
    "            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n",
    "                blocks.append(block)     # add it the blocks list\n",
    "                block = {}               # re-init the block\n",
    "            block[\"type\"] = line[1:-1].rstrip()     \n",
    "        else:\n",
    "            key,value = line.split(\"=\") \n",
    "            block[key.rstrip()] = value.lstrip()\n",
    "    blocks.append(block)\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(darknet.parse_cfg.__doc__)\n",
    "blocks = darknet.parse_cfg('cfg/yolov3.cfg')\n",
    "for cc in blocks:\n",
    "    print(cc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the building blocks\n",
    "Now we are going to use the list returned by the above parse_cfg to construct PyTorch modules for the blocks present in the config file.\n",
    "\n",
    "We have 5 types of layers in the list (mentioned above). PyTorch provides pre-built layers for types convolutional and upsample. We will have to write our own modules for the rest of the layers by extending the `nn.Module class`.\n",
    "\n",
    "Create a function **darknet.create_modules(blocks)**\n",
    "\n",
    "#### nn.ModuleList\n",
    "Our function will return a nn.ModuleList. This class is almost like a normal list containing nn.Module objects. However, when we add nn.ModuleList as a member of a nn.Module object (i.e. when we add modules to our network), all the parameters of nn.Module objects (modules) inside the nn.ModuleList are added as parameters of the nn.Module object (i.e. our network, which we are adding the nn.ModuleList as a member of) as well.\n",
    "\n",
    "#### prev_filter\n",
    "We need to keep track of number of filters in the layer on which the convolutional layer is being applied. We use the variable `prev_filter` to do this. We initialise this to 3, as the image has 3 filters corresponding to the RGB channels.\n",
    "\n",
    "#### output_filters\n",
    "The route layer brings (possibly concatenated) feature maps from previous layers. If there's a convolutional layer right in front of a route layer, then the kernel is applied on the feature maps of previous layers, precisely the ones the route layer brings. Therefore, we need to keep a track of the number of filters in not only the previous layer, but each one of the preceding layers. As we iterate, we append the number of output filters of each block to the list output_filters.\n",
    "\n",
    "    net_info = blocks[0]     # Captures the information about the input and pre-processing\n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(darknet.create_modules.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the list of blocks, and create a PyTorch module for each block as we go.\n",
    "\n",
    "#### nn.Sequential\n",
    "`nn.Sequential` class is used to sequentially execute a number of nn.Module objects. If you look at the cfg, you will realize a block may contain more than one layer. For example, a block of type convolutional has a batch norm layer as well as leaky ReLU activation layer in addition to a convolutional layer. We string together these layers using the nn.Sequential and the add_module function.\n",
    "\n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "\n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution layers\n",
    "    if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "\n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "\n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "\n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "\n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "\n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "\n",
    "#### Upsampling layers\n",
    "        #If it's an upsampling layer\n",
    "        #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"bilinear\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "            \n",
    "#### Route layer\n",
    "\n",
    "The Route Layer, just like any other layer performs an operation (bringing forward previous layer / concatenation). In PyTorch, when we define a new layer, we subclass `nn.Module` and write the operation the layer performs in the forward function of the nn.Module object.\n",
    "\n",
    "For designing a layer for the Route block, we will have to build a `nn.Module` object that is initialized with values of the attribute layers as it's member(s). Then, we can write the code to concatenate/bring forward the feature maps in the forward function. Finally, we then execute this layer in th `forward` function of our network.\n",
    "\n",
    "But given the code of concatenation is fairly short and simple (calling torch.cat on feature maps), designing a layer as above will lead to unnecessary abstraction that just increases boiler plate code. Instead, what we can do is put a dummy layer in place of a proposed route layer, and then perform the concatenation directly in the forward function of the nn.Module object representing darknet.\n",
    "\n",
    "The convolutional layer just in front of a route layer applies it's kernel to (possibly concatenated) feature maps from a previous layers. The following code updates the filters variable to hold the number of filters outputted by a route layer.\n",
    "\n",
    "    if end < 0:\n",
    "        #If we are concatenating maps\n",
    "        filters = output_filters[index + start] + output_filters[index + end]\n",
    "    else:\n",
    "        filters= output_filters[index + start]\n",
    "\n",
    "\n",
    "###### EmptyLayer\n",
    "\n",
    "    class EmptyLayer(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(EmptyLayer, self).__init__()\n",
    "\n",
    "###### Route Layer code\n",
    "\n",
    "        #If it is a route layer\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            #Start  of a route\n",
    "            start = int(x[\"layers\"][0])\n",
    "            #end, if there exists one.\n",
    "            try:\n",
    "                end = int(x[\"layers\"][1])\n",
    "            except:\n",
    "                end = 0\n",
    "            #Positive anotation\n",
    "            if start > 0: \n",
    "                start = start - index\n",
    "            if end > 0:\n",
    "                end = end - index\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "            if end < 0:\n",
    "                filters = output_filters[index + start] + output_filters[index + end]\n",
    "            else:\n",
    "                filters= output_filters[index + start]\n",
    "\n",
    "#### Shortcut layer\n",
    "\n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "#### YOLO layer (detection)\n",
    "\n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "\n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "\n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "\n",
    "###### DetectionLayer()\n",
    "We define a new layer DetectionLayer that holds the anchors used to detect bounding boxes.\n",
    "\n",
    "    class DetectionLayer(nn.Module):\n",
    "        def __init__(self, anchors):\n",
    "            super(DetectionLayer, self).__init__()\n",
    "            self.anchors = anchors\n",
    "            \n",
    "The whole function returns:\n",
    "\n",
    "    return (net_info, module_list)\n",
    "    \n",
    "### Test darknet.create_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(darknet)  # During dev only\n",
    "net_info, module_list = darknet.create_modules(blocks)\n",
    "print(\"NET_INFO:\")\n",
    "for key in net_info:\n",
    "    print(key, \":\", net_info[key])\n",
    "print()\n",
    "print(\"MODULE LIST\")\n",
    "for mod in module_list:\n",
    "    print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network\n",
    "Now we must define our network, using the `create_modules` function above.\n",
    "\n",
    "    class Darknet(nn.Module):\n",
    "        def __init__(self, cfgfile):\n",
    "            super(Darknet, self).__init__()\n",
    "            self.blocks = parse_cfg(cfgfile)\n",
    "            self.net_info, self.module_list = create_modules(self.blocks)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(darknet)  # During dev only\n",
    "dark = darknet.Darknet('cfg/yolov3.cfg')\n",
    "print(dark.__doc__)\n",
    "print('Number of blocks:', len(dark.blocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the forward pass of the network\n",
    "The forward pass of the network is implemented by overriding the `forward` method of the `nn.Module` class.\n",
    "\n",
    "`forward` serves two purposes. First, to calculate the output, and second, to transform the output detection feature maps in a way that it can be processed easier (such as transforming them such that detection maps across multiple scales can be concatenated, which otherwise isn't possible as they are of different dimensions).\n",
    "\n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "`forward` takes three arguments, self, the input x and CUDA, which if true, would use GPU to accelerate the forward pass.\n",
    "\n",
    "Here, we iterate over `self.blocks[1:]` instead of self.blocks since the first element of `self.blocks` is a net block which isn't a part of the forward pass.\n",
    "\n",
    "Since route and shortcut layers need output maps from previous layers, we cache the output feature maps of every layer in a dict outputs. The keys are the the indices of the layers, and the values are the feature maps.\n",
    "\n",
    "We now iterate over `module_list` which contains the modules of the network. The thing to notice here is that the modules have been appended in the same order as they are present in the configuration file. This means, we can simply run our input through each module to get our output.\n",
    "\n",
    "    write = 0     #This is explained a bit later\n",
    "    for i, module in enumerate(modules):        \n",
    "        module_type = (module[\"type\"])\n",
    "        \n",
    "Now we deal with each module by type\n",
    "\n",
    "#### Convolutional and upsample\n",
    "\n",
    "        if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "            x = self.module_list[i](x)\n",
    "            \n",
    "#### Route Layer / Shortcut Layer\n",
    "\n",
    "If you look the code for route layer, we have to account for two cases (as described in part 2). For the case in which we have to concatenate two feature maps we use the torch.cat function with the second argument as 1. This is because we want to concatenate the feature maps along the depth. (In PyTorch, input and output of a convolutional layer has the format batch x channels x H x W. The depth corresponding the the channel dimension).\n",
    "\n",
    "        elif module_type == \"route\":\n",
    "            layers = module[\"layers\"]\n",
    "            layers = [int(a) for a in layers]\n",
    "\n",
    "            if (layers[0]) > 0:\n",
    "                layers[0] = layers[0] - i\n",
    "\n",
    "            if len(layers) == 1:\n",
    "                x = outputs[i + (layers[0])]\n",
    "\n",
    "            else:\n",
    "                if (layers[1]) > 0:\n",
    "                    layers[1] = layers[1] - i\n",
    "\n",
    "                map1 = outputs[i + layers[0]]\n",
    "                map2 = outputs[i + layers[1]]\n",
    "\n",
    "                x = torch.cat((map1, map2), 1)\n",
    "\n",
    "        elif  module_type == \"shortcut\":\n",
    "            from_ = int(module[\"from\"])\n",
    "            x = outputs[i-1] + outputs[i+from_]\n",
    "\n",
    "#### YOLO (detection) layer\n",
    "\n",
    "The output of YOLO is a convolutional feature map that contains the bounding box attributes along the depth of the feature map. The attributes bounding boxes predicted by a cell are stacked one by one along each other. So, if you have to access the second bounding of cell at (5,6), then you will have to index it by `map[5,6, (5+C): 2*(5+C)]`. This form is very inconvenient for output processing such as thresholding by a object confidence, adding grid offsets to centers, applying anchors etc.\n",
    "\n",
    "Another problem is that since detections happen at three scales, the dimensions of the prediction maps will be different. Although the dimensions of the three feature maps are different, the output processing operations to be done on them are similar. It would be nice to have to do these operations on a single tensor, rather than three separate tensors.\n",
    "\n",
    "To remedy these problems, we introduce the function `predict_transform`\n",
    "\n",
    "### predict_transform\n",
    "\n",
    "The function `predict_transform` lives in the file util.py and we will import the function when we use it in forward of Darknet class.\n",
    "\n",
    "predict_transform takes in 5 parameters; prediction (our output), inp_dim (input image dimension), anchors, num_classes, and an optional CUDA flag\n",
    "\n",
    "    def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(util)\n",
    "print(util.predict_transform.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_transform` function takes an detection feature map and turns it into a 2-D tensor, where each row of the tensor corresponds to attributes of a bounding box, in the following order:\n",
    "\n",
    "- 1st bounding box at (0,0)\n",
    "- 2nd box at (0,0)\n",
    "- 3rd box at (0,0)\n",
    "- 1st box at (0,1)\n",
    "\n",
    "Code:\n",
    "\n",
    "    batch_size = prediction.size(0)\n",
    "    stride =  inp_dim // prediction.size(2)\n",
    "    grid_size = inp_dim // stride\n",
    "    bbox_attrs = 5 + num_classes\n",
    "    num_anchors = len(anchors)\n",
    "    \n",
    "    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n",
    "    prediction = prediction.transpose(1,2).contiguous()\n",
    "    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n",
    "    \n",
    "The dimensions of the anchors are in accordance to the height and width attributes of the net block. These attributes describe the dimensions of the input image, which is larger (by a factor of stride) than the detection map. Therefore, we must divide the anchors by the stride of the detection feature map.\n",
    "\n",
    "    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n",
    "    \n",
    "#### Transform the output\n",
    "\n",
    "Sigmoid the x,y coordinates and the objectness score.\n",
    "\n",
    "    #Sigmoid the  centre_X, centre_Y. and object confidencce\n",
    "    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n",
    "    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n",
    "    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n",
    "\n",
    "Add the grid offsets to the center cordinates prediction.\n",
    "\n",
    "    #Add the center offsets\n",
    "    grid = np.arange(grid_size)\n",
    "    a,b = np.meshgrid(grid, grid)\n",
    "\n",
    "    x_offset = torch.FloatTensor(a).view(-1,1)\n",
    "    y_offset = torch.FloatTensor(b).view(-1,1)\n",
    "\n",
    "    if CUDA:\n",
    "        x_offset = x_offset.cuda()\n",
    "        y_offset = y_offset.cuda()\n",
    "\n",
    "    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n",
    "\n",
    "    prediction[:,:,:2] += x_y_offset\n",
    "    \n",
    "Apply the anchors to the dimensions of the bounding box.\n",
    "\n",
    "    #log space transform height and the width\n",
    "    anchors = torch.FloatTensor(anchors)\n",
    "\n",
    "    if CUDA:\n",
    "        anchors = anchors.cuda()\n",
    "\n",
    "    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n",
    "    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n",
    "\n",
    "Apply sigmoid activation to the the class scores\n",
    "\n",
    "    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n",
    "    \n",
    "The last thing we want to do here, is to resize the detections map to the size of the input image. The bounding box attributes here are sized according to the feature map (say, 13 x 13). If the input image was 416 x 416, we multiply the attributes by 32, or the stride variable.\n",
    "\n",
    "    prediction[:,:,:4] *= stride\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection layer... again\n",
    "\n",
    "Now that we have transformed our output tensors, we can now concatenate the detection maps at three different scales into one big tensor. Notice this was not possible prior to our transformation, as one cannot concatenate feature maps having different spatial dimensions.\n",
    "\n",
    "#### YOLO (detection) layer... again\n",
    "\n",
    "        elif module_type == 'yolo':        \n",
    "\n",
    "            anchors = self.module_list[i][0].anchors\n",
    "            #Get the input dimensions\n",
    "            inp_dim = int (self.net_info[\"height\"])\n",
    "\n",
    "            #Get the number of classes\n",
    "            num_classes = int (module[\"classes\"])\n",
    "\n",
    "            #Transform \n",
    "            x = x.data\n",
    "            x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "            if not write:              #if no collector has been intialised. \n",
    "                detections = x\n",
    "                write = 1\n",
    "\n",
    "            else:       \n",
    "                detections = torch.cat((detections, x), 1)\n",
    "\n",
    "        outputs[i] = x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward pass of network\n",
    "\n",
    "Two solutions: First using the recommended code (cv2), second trying to do this in numpy / pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official code\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from  matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import darknet\n",
    "import util\n",
    "\n",
    "importlib.reload(util)\n",
    "importlib.reload(darknet)\n",
    "\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "#     img = cv2.imread(r\"/data/detection/images_examples/me_laura_hut.jpg\")\n",
    "    img = cv2.imread(r\"/data/detection/images_examples/dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim))           #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    return img_\n",
    "\n",
    "# def get_test_input(input_dim, CUDA):\n",
    "#     img = cv2.imread(\"pytorch_yolo_v3/dog-cycle-car.png\")\n",
    "#     img = cv2.resize(img, (input_dim, input_dim)) \n",
    "#     img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "#     img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "#     img_ = torch.from_numpy(img_).float()\n",
    "#     img_ = Variable(img_)\n",
    "    \n",
    "#     if CUDA:\n",
    "#         img_ = img_.cuda()\n",
    "#     num_classes\n",
    "#     return img_\n",
    "\n",
    "\n",
    "model = darknet.Darknet(\"cfg/yolov3.cfg\")\n",
    "# model.cuda()\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "model.eval()\n",
    "inp = get_test_input(608, use_cuda)\n",
    "print(inp.size())\n",
    "print(use_cuda)\n",
    "\n",
    "# print(next(model.parameters()).is_cuda)\n",
    "# print(inp.is_cuda)\n",
    "\n",
    "# pred = model(inp, torch.cuda.is_available())\n",
    "pred = model(inp, use_cuda)\n",
    "print(pred)\n",
    "print(pred.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading pre-trained weights\n",
    "\n",
    "Weights:\n",
    "\n",
    "    wget https://pjreddie.com/media/files/yolov3.weights\n",
    "    \n",
    "Saved to `~/dev_data/pretrained/yolov3/yolov3.weights`\n",
    "\n",
    "The official weights file is binary file that contains weights stored in a serial fashion.\n",
    "\n",
    "Extreme care must be taken to read the weights. The weights are just stored as floats, with nothing to guide us as to which layer do they belong to. If you screw up, there's nothing stopping you to, say, load the weights of a batch norm layer into those of a convolutional layer. Since, you're reading only floats, there's no way to discriminate between which weight belongs to which layer. Hence, we must understand how the weights are stored.\n",
    "\n",
    "First, the weights belong to only two types of layers, either a batch norm layer or a convolutional layer.\n",
    "\n",
    "The weights for these layers are stored exactly in the same order as they appear in the configuration file. So, if a convolutional is followed by a shortcut block, and then the shortcut block by another convolutional block, You will expect file to contain the weights of the previous convolutional block, followed by those of the latter.\n",
    "\n",
    "When the batch norm layer appears in a convolutional block, there are no biases. However, when there's no batch norm layer, bias \"weights\" have to read from the file.\n",
    "\n",
    "### Load weights\n",
    "\n",
    "Let us write a function load weights. It will be a member function of the `darknet.Darknet` class. It'll take one argument other than `self`, the path of the weightsfile.\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "    \n",
    "    #Open the weights file\n",
    "    fp = open(weightfile, \"rb\")\n",
    "\n",
    "    #The first 5 values are header information \n",
    "    # 1. Major version number\n",
    "    # 2. Minor Version Number\n",
    "    # 3. Subversion number \n",
    "    # 4,5. Images seen by the network (during training)\n",
    "    header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "    self.header = torch.from_numpy(header)\n",
    "    self.seen = self.header[3]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(darknet) # Dev only\n",
    "\n",
    "print(darknet.Darknet.load_weights.__doc__)\n",
    "\n",
    "model = darknet.Darknet(\"cfg/yolov3.cfg\")\n",
    "model.load_weights(r'/data/pretrained/yolov3/yolov3.weights')\n",
    "print(model.header[:3])\n",
    "print(model.seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of bits now represent the weights, in the order described above. The weights are stored as float32 or 32-bit floats. Let's load rest of the weights in a np.ndarray.\n",
    "\n",
    "    weights = np.fromfile(fp, dtype = np.float32)\n",
    "    \n",
    "Now, we iterate over the weights file, and load the weights into the modules of our network.\n",
    "\n",
    "    ptr = 0 # keep track of where we are in the weights array\n",
    "    for i in range(len(self.module_list)):\n",
    "        module_type = self.blocks[i + 1][\"type\"]\n",
    "\n",
    "        #If module_type is convolutional load weights\n",
    "        #Otherwise ignore.\n",
    "        \n",
    "Into the loop, we first check whether the convolutional block has batch_normalise True or not. Based on that, we load the weights.\n",
    "\n",
    "        if module_type == \"convolutional\":\n",
    "            model = self.module_list[i]\n",
    "            try:\n",
    "                batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "\n",
    "            conv = model[0]\n",
    "\n",
    "if batch_normalize is True, we load the weights as follows. If batch_norm is not true, simply load the biases of the convolutional layer.\n",
    "\n",
    "\n",
    "\n",
    "            if (batch_normalize):\n",
    "                bn = model[1]\n",
    "\n",
    "                #Get the number of weights of Batch Norm Layer\n",
    "                num_bn_biases = bn.bias.numel()\n",
    "\n",
    "                #Load the weights\n",
    "                bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                ptr += num_bn_biases\n",
    "\n",
    "                bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                ptr  += num_bn_biases\n",
    "\n",
    "                bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                ptr  += num_bn_biases\n",
    "\n",
    "                bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                ptr  += num_bn_biases\n",
    "\n",
    "                #Cast the loaded weights into dims of model weights. \n",
    "                bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "\n",
    "                #Copy the data to model\n",
    "                bn.bias.data.copy_(bn_biases)\n",
    "                bn.weight.data.copy_(bn_weights)\n",
    "                bn.running_mean.copy_(bn_running_mean)\n",
    "                bn.running_var.copy_(bn_running_var)\n",
    "\n",
    "            else:\n",
    "                #Number of biases\n",
    "                num_biases = conv.bias.numel()\n",
    "\n",
    "                #Load the weights\n",
    "                conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                ptr = ptr + num_biases\n",
    "\n",
    "                #reshape the loaded weights according to the dims of the model weights\n",
    "                conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "\n",
    "                #Finally copy the data\n",
    "                conv.bias.data.copy_(conv_biases)\n",
    "\n",
    "Now load the conv layer weights\n",
    "\n",
    "            #Let us load the weights for the Convolutional layers\n",
    "            num_weights = conv.weight.numel()\n",
    "\n",
    "            #Do the same as above for weights\n",
    "            conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "            ptr = ptr + num_weights\n",
    "\n",
    "            conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "            conv.weight.data.copy_(conv_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: Part four starts here**\n",
    "https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-4/\n",
    "\n",
    "We must subject our output to objectness score thresholding and Non-maximal suppression, to obtain what I will call in the rest of this post as the true detections. To do that, we will create a function called `write_results` in the file util.py\n",
    "\n",
    "    def write_results(prediction, confidence, num_classes, nms_conf = 0.4):\n",
    "    \n",
    "The functions takes as as input the prediction, confidence (objectness score threshold), num_classes (80, in our case) and nms_conf (the NMS IoU threshold).\n",
    "\n",
    "## write_results: object confidence supression\n",
    "### Supress all boxes with confidence < threshold\n",
    "\n",
    "For each of the bounding box having a objectness score below a threshold, we set the values of it's every attribute (entire row representing the bounding box) to zero.\n",
    "\n",
    "    conf_mask = (prediction[:,:,4] > confidence).float().unsqueeze(2)\n",
    "    prediction = prediction*conf_mask\n",
    "\n",
    "### Non-maximum supression\n",
    "The bounding box attributes we have now are described by the center coordinates, as well as the height and width of the bounding box. However, it's easier to calculate IoU of two boxes, using coordinates of a pair of diagnal corners of each box. So, we transform the (center x, center y, height, width) attributes of our boxes, to (top-left corner x, top-left corner y, right-bottom corner x, right-bottom corner y).\n",
    "\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n",
    "    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n",
    "    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n",
    "    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n",
    "    prediction[:,:,:4] = box_corner[:,:,:4]\n",
    "    \n",
    "The number of true detections in every image may be different. For example, a batch of size 3 where images 1, 2 and 3 have 5, 2, 4 true detections respectively. Therefore, confidence thresholding and NMS has to be done for one image at once. This means, we cannot vectorise the operations involved, and must loop over the first dimension of prediction (containing indexes of images in a batch).\n",
    "\n",
    "    batch_size = prediction.size(0)\n",
    "\n",
    "    write = False \n",
    "    # flag is used to indicate that we haven't initialized output, a tensor we will use to collect true detections across the entire batch.\n",
    "\n",
    "    for ind in range(batch_size):\n",
    "        image_pred = prediction[ind]          #image Tensor\n",
    "        #confidence threshholding \n",
    "        #NMS\n",
    "\n",
    "Each bounding box row has 85 attributes, out of which 80 are the class scores. At this point, we're only concerned with the class score having the maximum value. So, we remove the 80 class scores from each row, and instead add the index of the class having the maximum values, as well the class score of that class.\n",
    "\n",
    "*NB: Perhaps in the future I might wish to also know what the second class is, or the second class if confidence less than a threshold*\n",
    "\n",
    "        max_conf, max_conf_score = torch.max(image_pred[:,5:5+ num_classes], 1)\n",
    "        max_conf = max_conf.float().unsqueeze(1)\n",
    "        max_conf_score = max_conf_score.float().unsqueeze(1)\n",
    "        seq = (image_pred[:,:5], max_conf, max_conf_score)\n",
    "        image_pred = torch.cat(seq, 1) # This makes a much simpler tensor, with the five positional arguments,\n",
    "                                        # then the confidence (of the most confident class) and the arg of the most\n",
    "                                        # confidence class\n",
    "                                        \n",
    "Get rid of zeroed out rows\n",
    "\n",
    "        non_zero_ind = (torch.nonzero(image_pred[:,4]))\n",
    "        image_pred_ = image_pred[non_zero_ind.squeeze(),:].view(-1,7)\n",
    "        if image_pred_.shape[0] == 0:\n",
    "            continue # If no detections, stop considering this image.\n",
    "        \n",
    "Get the classes detected in a an image.\n",
    "\n",
    "        #Get the various classes detected in the image\n",
    "        img_classes = unique(image_pred_[:,-1]) # -1 index holds the class index\n",
    "        \n",
    "#### Aside: Unique function\n",
    "\n",
    "Since there can be multiple true detections of the same class, we use a function called unique to get classes present in any given image.\n",
    "\n",
    "    def unique(tensor):\n",
    "        tensor_np = tensor.cpu().numpy()\n",
    "        unique_np = np.unique(tensor_np)\n",
    "        unique_tensor = torch.from_numpy(unique_np)\n",
    "\n",
    "        tensor_res = tensor.new(unique_tensor.shape)\n",
    "        tensor_res.copy_(unique_tensor)\n",
    "        return tensor_res\n",
    "\n",
    "#### Back to NMS\n",
    "\n",
    "Loop through classes\n",
    "\n",
    "        for cls in img_classes:\n",
    "            #perform NMS\n",
    "            #get the detections with one particular class\n",
    "            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n",
    "            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n",
    "            image_pred_class = image_pred_[class_mask_ind].view(-1,7)\n",
    "\n",
    "            #sort the detections such that the entry with the maximum objectness\n",
    "            #confidence is at the top\n",
    "            conf_sort_index = torch.sort(image_pred_class[:,4], descending = True )[1]\n",
    "            image_pred_class = image_pred_class[conf_sort_index]\n",
    "            idx = image_pred_class.size(0)   #Number of detections\n",
    "            \n",
    "Now do non-maximum supression\n",
    "\n",
    "            for i in range(idx):\n",
    "                #Get the IOUs of all boxes that come after the one we are looking at \n",
    "                #in the loop\n",
    "                try:\n",
    "                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n",
    "                except ValueError:\n",
    "                    break\n",
    "\n",
    "                except IndexError:\n",
    "                    break\n",
    "\n",
    "                #Zero out all the detections that have IoU > treshhold\n",
    "                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n",
    "                image_pred_class[i+1:] *= iou_mask       \n",
    "\n",
    "                #Remove the non-zero entries\n",
    "                non_zero_ind = torch.nonzero(image_pred_class[:,4]).squeeze()\n",
    "                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n",
    "                \n",
    "#### Writing the predictions\n",
    "\n",
    "The function write_results outputs a tensor of shape D x 8. Here D is the true detections in all of images, each represented by a row. Each detections has 8 attributes, namely, index of the image in the batch to which the detection belongs to, 4 corner coordinates, objectness score, the score of class with maximum confidence, and the index of that class.\n",
    "\n",
    "Just as before, we do not initialize our output tensor unless we have a detection to assign to it. Once it has been initialized, we concatenate subsequent detections to it. We use the write flag to indicate whether the tensor has been initialized or not. At the end of loop that iterates over classes, we add the resultant detections to the tensor output.\n",
    "\n",
    "            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind)      \n",
    "            #Repeat the batch_id for as many detections of the class cls in the image\n",
    "            seq = batch_ind, image_pred_class\n",
    "\n",
    "            if not write:\n",
    "                output = torch.cat(seq,1)\n",
    "                write = True\n",
    "            else:\n",
    "                out = torch.cat(seq,1)\n",
    "                output = torch.cat((output,out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: Start of part five**\n",
    "https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-5/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(darknet) # Dev only\n",
    "importlib.reload(util)\n",
    "\n",
    "model = darknet.Darknet(\"cfg/yolov3.cfg\")\n",
    "model.load_weights(r'/data/pretrained/yolov3/yolov3.weights')\n",
    "\n",
    "inp = get_test_input(608, use_cuda)\n",
    "\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "model.eval()\n",
    "inp = get_test_input(608, use_cuda)\n",
    "\n",
    "pred = model(inp, use_cuda)\n",
    "\n",
    "output = util.write_results(pred, 0.5, 80, nms_conf=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.shape)\n",
    "print(output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the input and output pipelines.\n",
    "Tutorial uses `detect_tutorial.py`. My reduced version uses `detector.py`.\n",
    "\n",
    "`detector` is the module that future programmes will call.\n",
    "\n",
    "`detect_tutorial` uses argpass, but I'll replace this with a funtion, as I don't like argpass.\n",
    "\n",
    "### Read class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detector\n",
    "import torchvision\n",
    "importlib.reload(detector) # Dev only\n",
    "\n",
    "yolo = detector.YOLO()\n",
    "# print(yolo.class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create image dataset (for detector.YOLO.infer_write_dir method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1749: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This pred contains 7 objects\n",
      "Classes in this image: ['person', 'backpack']\n",
      "** IMAGE END **\n",
      "This pred contains 2 objects\n",
      "Classes in this image: ['person']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 4 objects\n",
      "Classes in this image: ['aeroplane']\n",
      "** IMAGE END **\n",
      "This pred contains 14 objects\n",
      "Classes in this image: ['backpack', 'truck', 'bus', 'person', 'suitcase']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "No detections for image water.jpg\n",
      "This pred contains 5 objects\n",
      "Classes in this image: ['person', 'chair', 'bowl', 'diningtable']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 8 objects\n",
      "Classes in this image: ['person', 'bottle']\n",
      "** IMAGE END **\n",
      "This pred contains 11 objects\n",
      "Classes in this image: ['person']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 2 objects\n",
      "Classes in this image: ['person', 'dog']\n",
      "** IMAGE END **\n",
      "This pred contains 6 objects\n",
      "Classes in this image: ['person', 'fire hydrant', 'car']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 1 objects\n",
      "Classes in this image: ['person']\n",
      "** IMAGE END **\n",
      "No detections for image sea.jpg\n",
      "***BATCH END***\n",
      "This pred contains 3 objects\n",
      "Classes in this image: ['truck', 'dog', 'bicycle']\n",
      "** IMAGE END **\n",
      "This pred contains 3 objects\n",
      "Classes in this image: ['car']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 10 objects\n",
      "Classes in this image: ['person', 'handbag']\n",
      "** IMAGE END **\n",
      "No detections for image tree.png\n",
      "***BATCH END***\n",
      "This pred contains 8 objects\n",
      "Classes in this image: ['person', 'tie', 'handbag']\n",
      "** IMAGE END **\n",
      "This pred contains 5 objects\n",
      "Classes in this image: ['person', 'truck', 'car']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n",
      "This pred contains 31 objects\n",
      "Classes in this image: ['person']\n",
      "** IMAGE END **\n",
      "This pred contains 2 objects\n",
      "Classes in this image: ['person', 'dog']\n",
      "** IMAGE END **\n",
      "***BATCH END***\n"
     ]
    }
   ],
   "source": [
    "import detector\n",
    "import importlib     # During dev only\n",
    "importlib.reload(detector) # Dev only\n",
    "\n",
    "yolo = detector.YOLO()\n",
    "yolo.infer_write_dir(r'/data/detection/images_examples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG: Make plotting function work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorbike',\n",
       " 'aeroplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'sofa',\n",
       " 'pottedplant',\n",
       " 'bed',\n",
       " 'diningtable',\n",
       " 'toilet',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import detector\n",
    "import importlib     # During dev only\n",
    "importlib.reload(detector) # Dev only\n",
    "\n",
    "yolo = detector.YOLO()\n",
    "yolo.class_names\n",
    "# yolo.infer_write_dir(r'/data/detection/images_examples')\n",
    "# (img, predictions, classes_str, class_colours, output_filepath) = yolo.infer_write_dir(r'/data/detection/images_examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'person', 'person', 'car', 'car', 'fire hydrant']\n"
     ]
    }
   ],
   "source": [
    "print(classes_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 911)\n"
     ]
    }
   ],
   "source": [
    "print(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour_map = { 'person': (198, 0, 0), \n",
    " 'bicycle': (164,125,70), \n",
    " 'car': (107,76,36), \n",
    " 'motorbike': (164,125,70), \n",
    " 'aeroplane': (92,92,92), \n",
    " 'bus': (69,48,23), \n",
    " 'train': (122, 41, 118), \n",
    " 'truck': (69,48,23), \n",
    " 'boat': (69,48,23), \n",
    " 'stop sign': (122, 41, 118), \n",
    " 'parking meter': (122, 41, 118), \n",
    " 'bird': (196,215,136), \n",
    " 'cat': (176,172,59), \n",
    " 'dog': (176,172,59), \n",
    " 'horse': (122, 41, 118), \n",
    " 'sheep': (122, 41, 118), \n",
    " 'cow': (122, 41, 118), \n",
    " 'elephant': (122, 41, 118), \n",
    " 'bear': (122, 41, 118), \n",
    " 'zebra': (122, 41, 118), \n",
    " 'giraffe': (122, 41, 118), \n",
    " 'backpack': (117, 157, 209), \n",
    " 'skateboard': (164,125,70), \n",
    " 'toilet': (122, 41, 118), \n",
    " 'suitcase': (57, 90, 172), \n",
    " 'tie': (57,90,172)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "def draw_output_images(img, predictions, classes_str, class_colours, output_filepath):\n",
    "    \"\"\"\n",
    "    Draw\n",
    "    :param img: PIL image\n",
    "    :param predictions: predictions, n x 8\n",
    "            0: batch pos (ignore), 1: x1, 2: y1, 3: x2, 4: y2, 5: box confidence, 6: class confidence, 7: class no.\n",
    "    :param classes_str: List of class stringss\n",
    "    :param class_colours: Dict relating class string to box colour. If not in, use 'default'\n",
    "    :param output_filepath: Path to save produced image to.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Get coordiantes\n",
    "    x1 = predictions[:, 1]\n",
    "    y1 = predictions[:, 2]\n",
    "    x2 = predictions[:, 3]\n",
    "    y2 = predictions[:, 4]\n",
    "    boxconf = predictions[:, 5]\n",
    "    classconf = predictions[:, 6]\n",
    "                \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for ii in range(x1.shape[0]):\n",
    "        this_class = classes_str[ii]\n",
    "        try:\n",
    "            colour = colour_map[this_class]\n",
    "        except KeyError:\n",
    "            colour = (138,138,138)\n",
    "        draw.rectangle((x1[ii],y1[ii],x2[ii],y2[ii]), fill=None, outline=colour)\n",
    "        \n",
    "        text = \"%s; %4.3f; %4.3f)\" % (this_class, boxconf[ii], classconf[ii])\n",
    "#         font=ImageFont.load_default()\n",
    "        font=ImageFont.truetype(font=r'cfg/FreeSansBold.ttf', size=13)\n",
    "        text_size = font.getsize(text)\n",
    "        draw.rectangle((x1[ii],y1[ii],x1[ii] + text_size[0],y1[ii] + text_size[1]), fill=\"black\")\n",
    "        draw.text((x1[ii], y1[ii]), text, fill='white', font=font)\n",
    "        \n",
    "        \n",
    "    img.save(output_filepath)\n",
    "    \n",
    "draw_output_images(img, predictions, classes_str, class_colours, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "def draw_output_images(img, predictions, classes_str, class_colours, output_filepath):\n",
    "    \"\"\"\n",
    "    Draw\n",
    "    :param img: PIL image\n",
    "    :param predictions: predictions, n x 8\n",
    "            0: batch pos (ignore), 1: x1, 2: y1, 3: x2, 4: y2, 5: box confidence, 6: class confidence, 7: class no.\n",
    "    :param classes_str: List of class stringss\n",
    "    :param class_colours: Dict relating class string to box colour. If not in, use 'default'\n",
    "    :param output_filepath: Path to save produced image to.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Get coordiantes\n",
    "    x1 = predictions[:, 1]\n",
    "    y1 = predictions[:, 2]\n",
    "    x2 = predictions[:, 3]\n",
    "    y2 = predictions[:, 4]\n",
    "    boxconf = predictions[:, 5]\n",
    "    classconf = predictions[:, 6]\n",
    "                \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for ii in range(x1.shape[0]):\n",
    "        points = (x1[ii], y1[ii]), (x2[ii], y1[ii]), (x2[ii], y2[ii]), (x1[ii], y2[ii]), (x1[ii], y1[ii])\n",
    "        \n",
    "        this_class = classes_str[ii]\n",
    "        try:\n",
    "            colour = colour_map[this_class]\n",
    "        except KeyError:\n",
    "            colour = (138,138,138)\n",
    "#         draw.rectangle((x1[ii],y1[ii],x2[ii],y2[ii]), fill=None, outline=colour)\n",
    "        draw.line(points, fill=colour, width=3)\n",
    "        text = \"%s; %4.3f; %4.3f)\" % (this_class, boxconf[ii], classconf[ii])\n",
    "#         font=ImageFont.load_default()\n",
    "        font=ImageFont.truetype(font=r'cfg/FreeSansBold.ttf', size=13)\n",
    "        text_size = font.getsize(text)\n",
    "        draw.rectangle((x1[ii] + 1,y1[ii] + 1,x1[ii] + 1 + text_size[0],y1[ii] + 1 + text_size[1]), fill=\"black\")\n",
    "        draw.text((x1[ii], y1[ii]), text, fill='white', font=font)\n",
    "        \n",
    "        \n",
    "    img.save(output_filepath)\n",
    "    \n",
    "draw_output_images(img, predictions, classes_str, class_colours, output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
