{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST autoencoder, focussing on the number of neurons in the dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input, Conv2D, MaxPooling2D, Conv2DTranspose, Reshape, concatenate \n",
    "from tensorflow.keras import utils, losses, optimizers, callbacks, regularizers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow import reduce_sum, abs as tf_abs, add as tf_add\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data, normalise, set to 0 or 1 (for logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "x_train = np.expand_dims(x_train, 3)\n",
    "x_test = np.expand_dims(x_test, 3)\n",
    "\n",
    "x_train[x_train>=0.5] = 1.0\n",
    "x_train[x_train<0.5] = 0\n",
    "\n",
    "x_test[x_test>=0.5] = 1.0\n",
    "x_test[x_test<0.5] = 0\n",
    "\n",
    "x_val = x_train[50000:, :, :, :]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000, :, :, :]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# One hot, in case we need it later\n",
    "num_classes = 10\n",
    "y_train_onehot = utils.to_categorical(y_train, num_classes)\n",
    "y_val_onehot = utils.to_categorical(y_val, num_classes)\n",
    "y_test_onehot = utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "data_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.543624569200343\n"
     ]
    }
   ],
   "source": [
    "ones = np.sum(x_train==1)\n",
    "zeros = np.sum(x_train==0)\n",
    "frac = ones / (zeros + ones)\n",
    "one_weighting = 1.0 / frac\n",
    "print(one_weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model, with varying numbers of dense nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_n(data, n_dense_nodes, dropout_amount):\n",
    "    \n",
    "    # DOWN bloc 1\n",
    "    c1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                     name='Conv_1', kernel_regularizer=regularizers.l2(0.01))(data)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', name='Conv_2', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c1)\n",
    "    p2 = MaxPooling2D(pool_size=(2, 2), name='Pool_2')(c2)\n",
    "    d2 = Dropout(dropout_amount, name='Dropout_2')(p2)\n",
    "    \n",
    "    # DOWN bloc 2\n",
    "    c3 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                     name='Conv_3', kernel_regularizer=regularizers.l2(0.01))(d2)\n",
    "    c4 = Conv2D(32, (3, 3), activation='relu', name='Conv_4', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c3)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2), name='Pool_4')(c4)\n",
    "    d4 = Dropout(dropout_amount, name='Dropout_4')(p4)\n",
    "    \n",
    "    # DOWN bloc 3\n",
    "    c5 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                     name='Conv_5', kernel_regularizer=regularizers.l2(0.01))(d4)\n",
    "    c6 = Conv2D(32, (3, 3), activation='relu', name='Conv_6', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c5)\n",
    "    d6 = Dropout(dropout_amount, name='Dropout_6')(c6)\n",
    "    \n",
    "    # Dense layer at bottom\n",
    "    f7 = Flatten(name='Flatten_7')(d6)\n",
    "    dense7 = Dense(n_dense_nodes, activation='relu', name='Dense_7')(f7)\n",
    "    dense8 = Dense(784, activation='relu', name='Dense_8')(dense7)\n",
    "    reshape8 = Reshape((7, 7, 16), name='Reshape_8')(dense8)\n",
    "    \n",
    "    # Upward bloc one\n",
    "    c10 = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                      name='Conv_10', kernel_regularizer=regularizers.l2(0.01))(reshape8)\n",
    "    c11 = Conv2D(16, (3, 3), activation='relu', name='Conv_11', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c10)\n",
    "    \n",
    "    # Upward bloc two\n",
    "    u12 = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', name='UpConv_12') (c11)\n",
    "    c13 = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                     name='Conv_13', kernel_regularizer=regularizers.l2(0.01))(u12)\n",
    "    c14 = Conv2D(16, (3, 3), activation='relu', name='Conv_14', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c13)\n",
    "\n",
    "    # Upward bloc three\n",
    "    u15 = Conv2DTranspose(16, (3, 3), strides=(2, 2), padding='same', name='UpConv_15') (c14)\n",
    "    c16 = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same', \n",
    "                     name='Conv_16', kernel_regularizer=regularizers.l2(0.01))(u15)\n",
    "    c17 = Conv2D(16, (3, 3), activation='relu', name='Conv_17', padding='same', \n",
    "                     kernel_regularizer=regularizers.l2(0.01))(c16)  \n",
    "\n",
    "    # Prep output\n",
    "    c18 = Conv2D(1, (1, 1), activation='relu', name='Conv_18', padding='same', \n",
    "                kernel_regularizer=regularizers.l2(0.01))(c17)\n",
    "    \n",
    "    outputs = c18\n",
    "    model = Model(inputs=[data], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dense_nodes = 98\n",
    "dropout_amount = 0.3\n",
    "desc = 'n-%i-drop-0p3' % n_dense_nodes\n",
    "\n",
    "input_img = Input(data_shape, name='img')\n",
    "model = model_n(input_img, n_dense_nodes, dropout_amount)\n",
    "model.compile(loss=losses.binary_crossentropy,\n",
    "              optimizer=optimizers.Adam(lr=0.00025), metrics=['mae'])\n",
    "# model.summary()\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto', \n",
    "                                    baseline=None)\n",
    "history = model.fit(x_train, x_train, batch_size=3072, epochs=200,\n",
    "          verbose=0, validation_data=(x_val, x_val), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = 'n-%i-drop-0p3' % n_dense_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot\n",
    "print(history.history.keys())\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss', fontsize=12)\n",
    "plt.xlabel('epoch', fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim([0, 1.5])\n",
    "plt.legend(['train', 'val', 'val_mae'], loc='upper left')\n",
    "plt.savefig('/data/imgs/mnist-autoencoder_%s_loss.png' % desc, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_preds = model.predict(x_val)\n",
    "\n",
    "examples = np.random.randint(0, high=x_val.shape[0], size=6)\n",
    "fig, axs = plt.subplots(nrows=6, ncols=2, figsize=(3.5,10))\n",
    "# fig.suptitle('Examples', fontsize=16)\n",
    "for ynum in range(6):\n",
    "    ax = axs[ynum, 0]\n",
    "    ax.imshow(x_val[examples[ynum], :, :, 0], cmap='gray')\n",
    "    ax.set_title('input', fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax = axs[ynum, 1]\n",
    "    ax.imshow(x_val_preds[examples[ynum], :, :, 0], cmap='gray')\n",
    "    ax.set_title('output', fontsize=12)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.savefig('/data/imgs/mnist-autoencoder_%s_in_out.png' % desc, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at weights of dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'Dense_7'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(x_val)\n",
    "print(intermediate_output.shape)\n",
    "### Run PCA and select the top 50 bands\n",
    "pca = PCA(n_components=50)\n",
    "trans_data = pca.fit_transform(intermediate_output)\n",
    "print(pca.explained_variance_ratio_[:10]) \n",
    "print(trans_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run t-sne and save output\n",
    "x_embedded = TSNE().fit_transform(trans_data)\n",
    "plt.figure()\n",
    "print(x_embedded.shape)\n",
    "plt.scatter(x_embedded[:, 0], x_embedded[:, 1], c=y_val, cmap='tab10')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.colorbar()\n",
    "plt.savefig('/data/imgs/mnist-autoencoder_%s_tsne.png' % desc, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply kmeans\n",
    "km_raw = KMeans(n_clusters=10).fit_predict(trans_data)\n",
    "cm = confusion_matrix(y_val, km_raw)\n",
    "# Create new labels, set all equal to 10\n",
    "km_labels = np.ones(km_raw.shape, dtype=np.int8) * 10\n",
    "\n",
    "## Calculate true km_label\n",
    "used = [False] * 10\n",
    "\n",
    "def assign_val(argorder, used, km_labels, km_raw):\n",
    "    for val in argorder:\n",
    "        if used[val] == False:\n",
    "            used[val] = True\n",
    "            print('True val:', true_val, 'k-means label:', val)\n",
    "            km_labels[km_raw==val] = true_val\n",
    "            return(used, km_labels, km_raw)\n",
    "            \n",
    "for true_val in range(10):\n",
    "    thisrow = cm[true_val, :]\n",
    "    argorder = np.argsort(-thisrow)\n",
    "    used, km_labels, km_raw = assign_val(argorder, used, km_labels, km_raw)\n",
    "    \n",
    "## Get confusion matrix and classification report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(confusion_matrix(y_val, km_labels))\n",
    "print(classification_report(y_val, km_labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
