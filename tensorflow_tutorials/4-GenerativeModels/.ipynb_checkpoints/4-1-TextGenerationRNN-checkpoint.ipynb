{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb\n",
    "\n",
    "#### This notebook demonstrates how to generate text using an RNN using tf.keras and eager execution. It uses Shakespeare's text, and predicts on a character-to-character basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (18.0)\n",
      "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow and enable eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "\n",
    "# Note: Once you enable eager execution, it cannot be disabled. \n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "text = unidecode.unidecode(open(path_to_file).read())\n",
    "# length of text is the number of characters in it\n",
    "print (len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick exploration of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "print(text[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionaries to map from characters to their indices and vice-versa, which will be used to vectorize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique contains all the unique characters in the file\n",
    "unique = sorted(set(text))\n",
    "\n",
    "# creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(unique)}\n",
    "idx2char = {i:u for i, u in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char[np.random.randint(65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the maximum length sentence we want for a single input in characters\n",
    "max_length = 100\n",
    "\n",
    "# length of the vocabulary in chars\n",
    "vocab_size = len(unique)\n",
    "\n",
    "# the embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN (here GRU) units\n",
    "units = 48 #1024 OOM\n",
    "\n",
    "# batch size \n",
    "BATCH_SIZE = 2 #64 gave OOM error\n",
    "\n",
    "# buffer size to shuffle our dataset\n",
    "BUFFER_SIZE = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input and output tensors \n",
    "\n",
    "We create *max_length* chunks of input, where each input vector is all the characters in that chunk apart from the last one. The target vector is all of the characters in the chunk except the first.\n",
    "\n",
    "eg if text = 'tensorflow' and max_length = 9:\n",
    "\n",
    "So, the input = 'tensorflo' and output = 'ensorflow'\n",
    "\n",
    "After creating the vectors, we convert each character into numbers using the char2idx dictionary we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11153, 100)\n",
      "(11153, 100)\n"
     ]
    }
   ],
   "source": [
    "input_text = []\n",
    "target_text = []\n",
    "\n",
    "for f in range(0, len(text)-max_length, max_length):\n",
    "    inps = text[f:f+max_length]\n",
    "    targ = text[f+1:f+1+max_length]\n",
    "\n",
    "    input_text.append([char2idx[i] for i in inps])\n",
    "    target_text.append([char2idx[t] for t in targ])\n",
    "    \n",
    "print (np.array(input_text).shape)\n",
    "print (np.array(target_text).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating batches and shuffling them using tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_text, target_text)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((2, 100), (2, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Model Subclassing API which gives us full flexibility to create the model and change it however we like. We use 3 layers to define our model.\n",
    "\n",
    "- Embedding layer\n",
    "- GRU layer (you can use an LSTM layer here)\n",
    "- Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_sz = batch_size\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if tf.test.is_gpu_available():\n",
    "            self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
    "                                          return_sequences=True, \n",
    "                                          return_state=True, \n",
    "                                          recurrent_initializer='glorot_uniform')\n",
    "#             self.gru = tf.keras.layers.GRU(self.units, \n",
    "#                                           return_sequences=True, \n",
    "#                                           return_state=True, \n",
    "#                                           recurrent_initializer='glorot_uniform')\n",
    "        else:\n",
    "            print(\"GPU not available!\")\n",
    "            raise NotImplementedError('No GPU')\n",
    "            self.gru = tf.keras.layers.GRU(self.units, \n",
    "                                         return_sequences=True, \n",
    "                                         return_state=True, \n",
    "                                         recurrent_activation='sigmoid', \n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # output shape == (batch_size, max_length, hidden_size) \n",
    "        # states shape == (batch_size, hidden_size)\n",
    "\n",
    "        # states variable to preserve the state of the model\n",
    "        # this will be used to pass at every step to the model while training\n",
    "        output, states = self.gru(x, initial_state=hidden)\n",
    "#         output, states = self.lstm(x, initial_state=hidden)\n",
    "\n",
    "\n",
    "        # reshaping the output so that we can pass it to the Dense layer\n",
    "        # after reshaping the shape is (batch_size * max_length, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # The dense layer will output predictions for every time_steps(max_length)\n",
    "        # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the model and set the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
    "def loss_function(real, preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Use a custom training loop with the help of GradientTape()\n",
    "\n",
    "- Initialize the hidden state of the model with zeros and shape = (batch_size, number of rnn units). We do this by calling the function defined while creating the model.\n",
    "- Iterate over the dataset(batch by batch) and calculate the predictions and the hidden states associated with that input.\n",
    "> There are a lot of interesting things happening here.\n",
    ">\n",
    ">    - The model gets hidden state(initialized with 0), lets call that H0 and the first batch of input, lets call that I0.\n",
    ">    - The model then returns the predictions P1 and H1.\n",
    ">    - For the next batch of input, the model receives I1 and H1.\n",
    ">    - The interesting thing here is that we pass H1 to the model with I1 which is how the model learns. The context learned from batch to batch is contained in the hidden state.\n",
    ">    - We continue doing this until the dataset is exhausted and then we start a new epoch and repeat this.\n",
    ">\n",
    "\n",
    "- After calculating the predictions, we calculate the loss using the loss function defined above. Then we calculate the gradients of the loss with respect to the model variables(input)\n",
    "- Finally, we take a step in that direction with the help of the optimizer using the apply_gradients function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.1950\n",
      "Epoch 1 Batch 1000 Loss 2.4477\n",
      "Epoch 1 Batch 2000 Loss 2.1120\n",
      "Epoch 1 Batch 3000 Loss 1.8236\n",
      "Epoch 1 Batch 4000 Loss 1.9768\n",
      "Epoch 1 Batch 5000 Loss 2.0564\n",
      "Epoch 1 Loss 1.8947\n",
      "Time taken for 1 epoch 67.89263153076172 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8408\n",
      "Epoch 2 Batch 1000 Loss 1.7586\n",
      "Epoch 2 Batch 2000 Loss 1.9422\n",
      "Epoch 2 Batch 3000 Loss 1.7976\n",
      "Epoch 2 Batch 4000 Loss 1.9543\n",
      "Epoch 2 Batch 5000 Loss 1.9162\n",
      "Epoch 2 Loss 1.8788\n",
      "Time taken for 1 epoch 68.19366955757141 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7400\n",
      "Epoch 3 Batch 1000 Loss 1.9125\n",
      "Epoch 3 Batch 2000 Loss 1.8218\n",
      "Epoch 3 Batch 3000 Loss 1.5420\n",
      "Epoch 3 Batch 4000 Loss 1.7195\n",
      "Epoch 3 Batch 5000 Loss 1.8223\n",
      "Epoch 3 Loss 1.7167\n",
      "Time taken for 1 epoch 67.27204251289368 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.7982\n",
      "Epoch 4 Batch 1000 Loss 1.7003\n",
      "Epoch 4 Batch 2000 Loss 1.6080\n",
      "Epoch 4 Batch 3000 Loss 1.5928\n",
      "Epoch 4 Batch 4000 Loss 1.5555\n",
      "Epoch 4 Batch 5000 Loss 1.8167\n",
      "Epoch 4 Loss 1.7226\n",
      "Time taken for 1 epoch 67.7898895740509 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7029\n",
      "Epoch 5 Batch 1000 Loss 1.5916\n",
      "Epoch 5 Batch 2000 Loss 1.7182\n",
      "Epoch 5 Batch 3000 Loss 1.8611\n",
      "Epoch 5 Batch 4000 Loss 1.6466\n",
      "Epoch 5 Batch 5000 Loss 1.7389\n",
      "Epoch 5 Loss 1.9628\n",
      "Time taken for 1 epoch 67.8353624343872 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.9259\n",
      "Epoch 6 Batch 1000 Loss 1.7714\n",
      "Epoch 6 Batch 2000 Loss 1.5804\n",
      "Epoch 6 Batch 3000 Loss 1.6701\n",
      "Epoch 6 Batch 4000 Loss 1.6889\n",
      "Epoch 6 Batch 5000 Loss 1.8044\n",
      "Epoch 6 Loss 1.6609\n",
      "Time taken for 1 epoch 67.91815853118896 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.7738\n",
      "Epoch 7 Batch 1000 Loss 1.6593\n",
      "Epoch 7 Batch 2000 Loss 1.6018\n",
      "Epoch 7 Batch 3000 Loss 1.7116\n",
      "Epoch 7 Batch 4000 Loss 1.6031\n",
      "Epoch 7 Batch 5000 Loss 1.5971\n",
      "Epoch 7 Loss 1.6407\n",
      "Time taken for 1 epoch 67.56324768066406 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.6290\n",
      "Epoch 8 Batch 1000 Loss 1.8811\n",
      "Epoch 8 Batch 2000 Loss 1.6082\n",
      "Epoch 8 Batch 3000 Loss 1.8795\n",
      "Epoch 8 Batch 4000 Loss 1.7879\n",
      "Epoch 8 Batch 5000 Loss 1.6503\n",
      "Epoch 8 Loss 1.7786\n",
      "Time taken for 1 epoch 67.91440677642822 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.7300\n",
      "Epoch 9 Batch 1000 Loss 1.5844\n",
      "Epoch 9 Batch 2000 Loss 1.8259\n",
      "Epoch 9 Batch 3000 Loss 1.6548\n",
      "Epoch 9 Batch 4000 Loss 1.8280\n",
      "Epoch 9 Batch 5000 Loss 1.6993\n",
      "Epoch 9 Loss 1.6263\n",
      "Time taken for 1 epoch 67.81740617752075 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.6357\n",
      "Epoch 10 Batch 1000 Loss 1.3888\n",
      "Epoch 10 Batch 2000 Loss 1.7408\n",
      "Epoch 10 Batch 3000 Loss 1.7018\n",
      "Epoch 10 Batch 4000 Loss 1.6709\n",
      "Epoch 10 Batch 5000 Loss 1.7540\n",
      "Epoch 10 Loss 1.7166\n",
      "Time taken for 1 epoch 67.79512858390808 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.5892\n",
      "Epoch 11 Batch 1000 Loss 1.9011\n",
      "Epoch 11 Batch 2000 Loss 1.6100\n",
      "Epoch 11 Batch 3000 Loss 1.7489\n",
      "Epoch 11 Batch 4000 Loss 1.7292\n",
      "Epoch 11 Batch 5000 Loss 1.6571\n",
      "Epoch 11 Loss 1.8200\n",
      "Time taken for 1 epoch 67.36274003982544 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.6662\n",
      "Epoch 12 Batch 1000 Loss 1.3652\n",
      "Epoch 12 Batch 2000 Loss 1.5886\n",
      "Epoch 12 Batch 3000 Loss 1.4577\n",
      "Epoch 12 Batch 4000 Loss 1.5862\n",
      "Epoch 12 Batch 5000 Loss 1.3485\n",
      "Epoch 12 Loss 1.7180\n",
      "Time taken for 1 epoch 67.32348227500916 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.6250\n",
      "Epoch 13 Batch 1000 Loss 1.6933\n",
      "Epoch 13 Batch 2000 Loss 1.7074\n",
      "Epoch 13 Batch 3000 Loss 1.6715\n",
      "Epoch 13 Batch 4000 Loss 1.9579\n",
      "Epoch 13 Batch 5000 Loss 1.7263\n",
      "Epoch 13 Loss 1.7184\n",
      "Time taken for 1 epoch 67.70960068702698 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.6996\n",
      "Epoch 14 Batch 1000 Loss 1.7179\n",
      "Epoch 14 Batch 2000 Loss 1.7827\n",
      "Epoch 14 Batch 3000 Loss 1.7234\n",
      "Epoch 14 Batch 4000 Loss 1.7769\n",
      "Epoch 14 Batch 5000 Loss 1.5853\n",
      "Epoch 14 Loss 1.7263\n",
      "Time taken for 1 epoch 67.39414691925049 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.6193\n",
      "Epoch 15 Batch 1000 Loss 1.6914\n",
      "Epoch 15 Batch 2000 Loss 1.4656\n",
      "Epoch 15 Batch 3000 Loss 1.7161\n",
      "Epoch 15 Batch 4000 Loss 1.7410\n",
      "Epoch 15 Batch 5000 Loss 1.7776\n",
      "Epoch 15 Loss 1.5859\n",
      "Time taken for 1 epoch 67.55303359031677 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.9108\n",
      "Epoch 16 Batch 1000 Loss 1.6793\n",
      "Epoch 16 Batch 2000 Loss 1.8502\n",
      "Epoch 16 Batch 3000 Loss 1.6668\n",
      "Epoch 16 Batch 4000 Loss 1.6383\n",
      "Epoch 16 Batch 5000 Loss 1.5618\n",
      "Epoch 16 Loss 1.7567\n",
      "Time taken for 1 epoch 67.7604112625122 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.8097\n",
      "Epoch 17 Batch 1000 Loss 1.5629\n",
      "Epoch 17 Batch 2000 Loss 1.7440\n",
      "Epoch 17 Batch 3000 Loss 1.6894\n",
      "Epoch 17 Batch 4000 Loss 2.2154\n",
      "Epoch 17 Batch 5000 Loss 1.8435\n",
      "Epoch 17 Loss 2.0044\n",
      "Time taken for 1 epoch 67.2962965965271 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.7266\n",
      "Epoch 18 Batch 1000 Loss 1.6521\n",
      "Epoch 18 Batch 2000 Loss 1.7049\n",
      "Epoch 18 Batch 3000 Loss 1.8051\n",
      "Epoch 18 Batch 4000 Loss 1.5750\n",
      "Epoch 18 Batch 5000 Loss 1.7073\n",
      "Epoch 18 Loss 1.4757\n",
      "Time taken for 1 epoch 67.16085267066956 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.5779\n",
      "Epoch 19 Batch 1000 Loss 1.6172\n",
      "Epoch 19 Batch 2000 Loss 1.7996\n",
      "Epoch 19 Batch 3000 Loss 1.6060\n",
      "Epoch 19 Batch 4000 Loss 1.4939\n",
      "Epoch 19 Batch 5000 Loss 1.4849\n",
      "Epoch 19 Loss 1.5349\n",
      "Time taken for 1 epoch 67.49886131286621 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.6560\n",
      "Epoch 20 Batch 1000 Loss 1.6664\n",
      "Epoch 20 Batch 2000 Loss 1.6262\n",
      "Epoch 20 Batch 3000 Loss 1.7106\n",
      "Epoch 20 Batch 4000 Loss 1.5212\n",
      "Epoch 20 Batch 5000 Loss 1.6244\n",
      "Epoch 20 Loss 1.8481\n",
      "Time taken for 1 epoch 67.2779471874237 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "\n",
    "EPOCHS = 20\n",
    "# EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch, (inp, target)) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "              # feeding the hidden state back into the model\n",
    "              # This is the interesting step\n",
    "            predictions, hidden = model(inp, hidden)\n",
    "              \n",
    "              # reshaping the target because that's how the \n",
    "              # loss function expects it\n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    "              \n",
    "        grads = tape.gradient(loss, model.variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
    "                                                    batch,\n",
    "                                                    loss))\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7fe58c039828>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To: begnother, letter seece\n",
      "Nay, hegor is the pant relace,\n",
      "A have fish, busit wit e to Capoll that ot;\n",
      "And or, they Boodig'ld Banget, stroke.\n",
      "Ime, art nugs oah.\n",
      "\n",
      "FRIAR ANNES:\n",
      "Held iffildy: his some?\n",
      "\n",
      "HAMONSIO:\n",
      "Shall the dood and I henose's iline,\n",
      "But these till be countran:\n",
      "Monrour Manchoses and the mine cloovor the rememons\n",
      "My taince for it and spoke to jurd is her sweet\n",
      "WhiUS\n",
      "At cleast the plascle,\n",
      "Fret from forly digntian.\n",
      "Ainsman of had will ding; I mave prese.\n",
      "\n",
      "BUCKINGHAM:\n",
      "My pleagred gors, an Gild with Sleasteds Glost bloy, hope she meet,\n",
      "She set on Macleming all being him,\n",
      "Now I gond know not thou dounds of sworn\n",
      "To post: thapother mean she the wan not is\n",
      "Pargament he pleages is to must my malmiels.\n",
      "The are of vick but a crangumineal\n",
      "To sters, it beciribremy, for my dislemmed.\n",
      "\n",
      "GROSPE:\n",
      "And pertany all you to presicite, look all:\n",
      "And he sneother.\n",
      "\n",
      "SICINIUS:\n",
      "Who fellay, the jery against thus for then, I did thee, wa;\n",
      "As fish them noble thee our laceswe command\n",
      "Is ten and no reewope\n"
     ]
    }
   ],
   "source": [
    "# Evaluation step(generating text using the model learned)\n",
    "\n",
    "# number of characters to generate\n",
    "num_generate = 1000\n",
    "\n",
    "# You can change the start string to experiment\n",
    "start_string = 'F'\n",
    "start_string = idx2char[np.random.randint(65)]\n",
    "\n",
    "# converting our start string to numbers(vectorizing!) \n",
    "input_eval = [char2idx[s] for s in start_string]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "# empty string to store our results\n",
    "text_generated = ''\n",
    "\n",
    "# low temperatures results in more predictable text.\n",
    "# higher temperatures results in more surprising text\n",
    "# experiment to find the best setting\n",
    "temperature = 1.0\n",
    "\n",
    "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
    "hidden = [tf.zeros((1, units))]\n",
    "for i in range(num_generate):\n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "\n",
    "    # using a multinomial distribution to predict the word returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
    "    \n",
    "    # We pass the predicted word as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    text_generated += idx2char[predicted_id]\n",
    "\n",
    "print (start_string + text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
