{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation with Attention\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\n",
    "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using tf.keras and eager execution. This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
    "\n",
    "After training the model in this notebook, you will be able to input a Spanish sentence, such as \"¿todavia estan en casa?\", and return the English translation: \"are you still at home?\"\n",
    "\n",
    "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare the dataset\n",
    "\n",
    "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
    "\n",
    "May I borrow this book?    ¿Puedo tomar prestado este libro?\n",
    "\n",
    "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
    "\n",
    "- Add a start and end token to each sentence.\n",
    "- Clean the sentences by removing special characters.\n",
    "- Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
    "- Pad each sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://download.tensorflow.org/data/spa-eng.zip\n",
      "2646016/2638744 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    '/data/tmp/spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return word_pairs\n",
    "\n",
    "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "  def __init__(self, lang):\n",
    "    self.lang = lang\n",
    "    self.word2idx = {}\n",
    "    self.idx2word = {}\n",
    "    self.vocab = set()\n",
    "    \n",
    "    self.create_index()\n",
    "    \n",
    "  def create_index(self):\n",
    "    for phrase in self.lang:\n",
    "      self.vocab.update(phrase.split(' '))\n",
    "    \n",
    "    self.vocab = sorted(self.vocab)\n",
    "    \n",
    "    self.word2idx['<pad>'] = 0\n",
    "    for index, word in enumerate(self.vocab):\n",
    "      self.word2idx[word] = index + 1\n",
    "    \n",
    "    for word, index in self.word2idx.items():\n",
    "      self.idx2word[index] = word\n",
    "    \n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def load_dataset(path, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    pairs = create_dataset(path, num_examples)\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(sp for en, sp in pairs)\n",
    "    targ_lang = LanguageIndex(en for en, sp in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # Spanish sentences\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # English sentences\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n",
    "    \n",
    "    # Calculate max_length of input and output tensor\n",
    "    # Here, we'll set those to the longest sentence in the dataset\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # Padding the input and output tensor to the maximum length\n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n",
    "                                                                 maxlen=max_length_inp,\n",
    "                                                                 padding='post')\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n",
    "                                                                  maxlen=max_length_tar, \n",
    "                                                                  padding='post')\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: limit the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000, 2000, 2000)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 15000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: <start> tom es un perdedor . <end> <pad> <pad> <pad> <pad> <pad>\n",
      "English: <start> tom is a loser . <end> <pad>\n"
     ]
    }
   ],
   "source": [
    "example_idx = np.random.randint(len(input_tensor_train))\n",
    "spanish = ' '.join([inp_lang.idx2word[w] for w in input_tensor_train[example_idx]])\n",
    "english = ' '.join([targ_lang.idx2word[w] for w in target_tensor_train[example_idx]])\n",
    "print('Spanish:', spanish)\n",
    "print('English:', english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "## Overwrite parameters due to memory issues\n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 1\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 48\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using Bahdanau attention. Lets decide on notation before writing the simplified form:\n",
    "\n",
    "- FC = Fully connected (dense) layer\n",
    "- EO = Encoder output\n",
    "- H = hidden state\n",
    "- X = input to the decoder\n",
    "\n",
    "And the pseudo-code:\n",
    "\n",
    "- score = FC(tanh(FC(EO) + FC(H)))\n",
    "- attention weights = softmax(score, axis = 1). Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "- context vector = sum(attention weights * EO, axis = 1). Same reason as above for choosing axis as 1.\n",
    "- embedding output = The input to the decoder X is passed through an embedding layer.\n",
    "- merged vector = concat(embedding output, context vector)\n",
    "This merged vector is then given to the GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n",
    "  # the code automatically does that.\n",
    "  if tf.test.is_gpu_available():\n",
    "    return tf.keras.layers.CuDNNGRU(units, \n",
    "                                    return_sequences=True, \n",
    "                                    return_state=True, \n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "  else:\n",
    "    return tf.keras.layers.GRU(units, \n",
    "                               return_sequences=True, \n",
    "                               return_state=True, \n",
    "                               recurrent_activation='sigmoid', \n",
    "                               recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = 1 - np.equal(real, 0)\n",
    "  loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/data/tmp/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pass the input through the encoder which return encoder output and the encoder hidden state.\n",
    "2. The encoder output, encoder hidden state and the decoder input (which is the start token) is passed to the decoder.\n",
    "3. The decoder returns the predictions and the decoder hidden state.\n",
    "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
    "5. Use teacher forcing to decide the next input to the decoder.\n",
    "6. Teacher forcing is the technique where the target word is passed as the next input to the decoder.\n",
    "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.9897\n",
      "Epoch 1 Batch 100 Loss 1.3335\n",
      "Epoch 1 Batch 200 Loss 0.8904\n",
      "Epoch 1 Batch 300 Loss 0.8897\n",
      "Epoch 1 Batch 400 Loss 1.1279\n",
      "Epoch 1 Batch 500 Loss 1.7093\n",
      "Epoch 1 Batch 600 Loss 1.6678\n",
      "Epoch 1 Batch 700 Loss 1.1833\n",
      "Epoch 1 Batch 800 Loss 0.9125\n",
      "Epoch 1 Batch 900 Loss 0.9556\n",
      "Epoch 1 Batch 1000 Loss 1.1936\n",
      "Epoch 1 Batch 1100 Loss 1.2137\n",
      "Epoch 1 Batch 1200 Loss 1.3539\n",
      "Epoch 1 Batch 1300 Loss 0.9158\n",
      "Epoch 1 Batch 1400 Loss 1.1657\n",
      "Epoch 1 Batch 1500 Loss 1.4911\n",
      "Epoch 1 Batch 1600 Loss 0.6723\n",
      "Epoch 1 Batch 1700 Loss 1.1018\n",
      "Epoch 1 Batch 1800 Loss 1.0640\n",
      "Epoch 1 Batch 1900 Loss 1.8484\n",
      "Epoch 1 Batch 2000 Loss 1.4142\n",
      "Epoch 1 Batch 2100 Loss 0.8755\n",
      "Epoch 1 Batch 2200 Loss 1.3420\n",
      "Epoch 1 Batch 2300 Loss 0.9211\n",
      "Epoch 1 Batch 2400 Loss 1.1922\n",
      "Epoch 1 Batch 2500 Loss 1.0782\n",
      "Epoch 1 Batch 2600 Loss 0.7846\n",
      "Epoch 1 Batch 2700 Loss 1.0941\n",
      "Epoch 1 Batch 2800 Loss 1.2993\n",
      "Epoch 1 Batch 2900 Loss 1.3532\n",
      "Epoch 1 Batch 3000 Loss 2.5322\n",
      "Epoch 1 Batch 3100 Loss 0.6309\n",
      "Epoch 1 Batch 3200 Loss 2.2073\n",
      "Epoch 1 Batch 3300 Loss 1.1086\n",
      "Epoch 1 Batch 3400 Loss 0.5616\n",
      "Epoch 1 Batch 3500 Loss 1.3071\n",
      "Epoch 1 Batch 3600 Loss 1.3168\n",
      "Epoch 1 Batch 3700 Loss 1.3944\n",
      "Epoch 1 Batch 3800 Loss 1.6133\n",
      "Epoch 1 Batch 3900 Loss 1.3229\n",
      "Epoch 1 Batch 4000 Loss 2.1060\n",
      "Epoch 1 Batch 4100 Loss 0.7307\n",
      "Epoch 1 Batch 4200 Loss 1.6296\n",
      "Epoch 1 Batch 4300 Loss 0.9411\n",
      "Epoch 1 Batch 4400 Loss 0.9980\n",
      "Epoch 1 Batch 4500 Loss 0.6664\n",
      "Epoch 1 Batch 4600 Loss 0.8792\n",
      "Epoch 1 Batch 4700 Loss 2.1349\n",
      "Epoch 1 Batch 4800 Loss 1.4502\n",
      "Epoch 1 Batch 4900 Loss 0.7742\n",
      "Epoch 1 Batch 5000 Loss 1.5654\n",
      "Epoch 1 Batch 5100 Loss 0.8007\n",
      "Epoch 1 Batch 5200 Loss 1.2069\n",
      "Epoch 1 Batch 5300 Loss 1.5160\n",
      "Epoch 1 Batch 5400 Loss 0.9396\n",
      "Epoch 1 Batch 5500 Loss 0.3575\n",
      "Epoch 1 Batch 5600 Loss 1.1478\n",
      "Epoch 1 Batch 5700 Loss 0.9832\n",
      "Epoch 1 Batch 5800 Loss 1.1763\n",
      "Epoch 1 Batch 5900 Loss 0.7564\n",
      "Epoch 1 Batch 6000 Loss 0.9082\n",
      "Epoch 1 Batch 6100 Loss 0.9484\n",
      "Epoch 1 Batch 6200 Loss 1.2192\n",
      "Epoch 1 Batch 6300 Loss 1.3307\n",
      "Epoch 1 Batch 6400 Loss 0.6895\n",
      "Epoch 1 Batch 6500 Loss 1.1338\n",
      "Epoch 1 Batch 6600 Loss 0.8462\n",
      "Epoch 1 Batch 6700 Loss 0.6339\n",
      "Epoch 1 Batch 6800 Loss 1.6817\n",
      "Epoch 1 Batch 6900 Loss 0.7611\n",
      "Epoch 1 Batch 7000 Loss 2.0668\n",
      "Epoch 1 Batch 7100 Loss 1.7326\n",
      "Epoch 1 Batch 7200 Loss 0.7543\n",
      "Epoch 1 Batch 7300 Loss 1.3163\n",
      "Epoch 1 Batch 7400 Loss 0.8082\n",
      "Epoch 1 Batch 7500 Loss 1.2237\n",
      "Epoch 1 Batch 7600 Loss 1.6227\n",
      "Epoch 1 Batch 7700 Loss 1.0277\n",
      "Epoch 1 Batch 7800 Loss 1.2566\n",
      "Epoch 1 Batch 7900 Loss 1.3647\n",
      "Epoch 1 Loss 1.2258\n",
      "Time taken for 1 epoch 848.3976495265961 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5478\n",
      "Epoch 2 Batch 100 Loss 0.7743\n",
      "Epoch 2 Batch 200 Loss 2.0411\n",
      "Epoch 2 Batch 300 Loss 1.7608\n",
      "Epoch 2 Batch 400 Loss 1.1446\n",
      "Epoch 2 Batch 500 Loss 0.9496\n",
      "Epoch 2 Batch 600 Loss 0.9210\n",
      "Epoch 2 Batch 700 Loss 1.4026\n",
      "Epoch 2 Batch 800 Loss 0.7351\n",
      "Epoch 2 Batch 900 Loss 0.3416\n",
      "Epoch 2 Batch 1000 Loss 1.3993\n",
      "Epoch 2 Batch 1100 Loss 0.6676\n",
      "Epoch 2 Batch 1200 Loss 1.1304\n",
      "Epoch 2 Batch 1300 Loss 1.1803\n",
      "Epoch 2 Batch 1400 Loss 0.7225\n",
      "Epoch 2 Batch 1500 Loss 1.0227\n",
      "Epoch 2 Batch 1600 Loss 1.0730\n",
      "Epoch 2 Batch 1700 Loss 1.2888\n",
      "Epoch 2 Batch 1800 Loss 1.3625\n",
      "Epoch 2 Batch 1900 Loss 1.8776\n",
      "Epoch 2 Batch 2000 Loss 0.8376\n",
      "Epoch 2 Batch 2100 Loss 1.2990\n",
      "Epoch 2 Batch 2200 Loss 1.3127\n",
      "Epoch 2 Batch 2300 Loss 0.9699\n",
      "Epoch 2 Batch 2400 Loss 1.2281\n",
      "Epoch 2 Batch 2500 Loss 1.1067\n",
      "Epoch 2 Batch 2600 Loss 1.0133\n",
      "Epoch 2 Batch 2700 Loss 1.2516\n",
      "Epoch 2 Batch 2800 Loss 1.1012\n",
      "Epoch 2 Batch 2900 Loss 0.7397\n",
      "Epoch 2 Batch 3000 Loss 0.9048\n",
      "Epoch 2 Batch 3100 Loss 1.1611\n",
      "Epoch 2 Batch 3200 Loss 1.6160\n",
      "Epoch 2 Batch 3300 Loss 1.2937\n",
      "Epoch 2 Batch 3400 Loss 1.0315\n",
      "Epoch 2 Batch 3500 Loss 1.2784\n",
      "Epoch 2 Batch 3600 Loss 0.6213\n",
      "Epoch 2 Batch 3700 Loss 1.6650\n",
      "Epoch 2 Batch 3800 Loss 1.0345\n",
      "Epoch 2 Batch 3900 Loss 0.6058\n",
      "Epoch 2 Batch 4000 Loss 1.2265\n",
      "Epoch 2 Batch 4100 Loss 0.5979\n",
      "Epoch 2 Batch 4200 Loss 1.3037\n",
      "Epoch 2 Batch 4300 Loss 0.8109\n",
      "Epoch 2 Batch 4400 Loss 1.0119\n",
      "Epoch 2 Batch 4500 Loss 0.8859\n",
      "Epoch 2 Batch 4600 Loss 0.9309\n",
      "Epoch 2 Batch 4700 Loss 1.3280\n",
      "Epoch 2 Batch 4800 Loss 0.8485\n",
      "Epoch 2 Batch 4900 Loss 2.0424\n",
      "Epoch 2 Batch 5000 Loss 1.5030\n",
      "Epoch 2 Batch 5100 Loss 0.6125\n",
      "Epoch 2 Batch 5200 Loss 1.0236\n",
      "Epoch 2 Batch 5300 Loss 1.0261\n",
      "Epoch 2 Batch 5400 Loss 1.3390\n",
      "Epoch 2 Batch 5500 Loss 1.3930\n",
      "Epoch 2 Batch 5600 Loss 1.7724\n",
      "Epoch 2 Batch 5700 Loss 1.1134\n",
      "Epoch 2 Batch 5800 Loss 0.7414\n",
      "Epoch 2 Batch 5900 Loss 1.6436\n",
      "Epoch 2 Batch 6000 Loss 0.7636\n",
      "Epoch 2 Batch 6100 Loss 0.9005\n",
      "Epoch 2 Batch 6200 Loss 1.2288\n",
      "Epoch 2 Batch 6300 Loss 1.1319\n",
      "Epoch 2 Batch 6400 Loss 1.3387\n",
      "Epoch 2 Batch 6500 Loss 0.3756\n",
      "Epoch 2 Batch 6600 Loss 0.9233\n",
      "Epoch 2 Batch 6700 Loss 0.7490\n",
      "Epoch 2 Batch 6800 Loss 1.3479\n",
      "Epoch 2 Batch 6900 Loss 0.6665\n",
      "Epoch 2 Batch 7000 Loss 1.2513\n",
      "Epoch 2 Batch 7100 Loss 1.9710\n",
      "Epoch 2 Batch 7200 Loss 1.7198\n",
      "Epoch 2 Batch 7300 Loss 1.6224\n",
      "Epoch 2 Batch 7400 Loss 0.7393\n",
      "Epoch 2 Batch 7500 Loss 1.1613\n",
      "Epoch 2 Batch 7600 Loss 0.8236\n",
      "Epoch 2 Batch 7700 Loss 1.6606\n",
      "Epoch 2 Batch 7800 Loss 1.4029\n",
      "Epoch 2 Batch 7900 Loss 1.0699\n",
      "Epoch 2 Loss 1.1414\n",
      "Time taken for 1 epoch 739.1677424907684 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.8059\n",
      "Epoch 3 Batch 100 Loss 0.7319\n",
      "Epoch 3 Batch 200 Loss 1.2838\n",
      "Epoch 3 Batch 300 Loss 2.5060\n",
      "Epoch 3 Batch 400 Loss 1.0535\n",
      "Epoch 3 Batch 500 Loss 1.0897\n",
      "Epoch 3 Batch 600 Loss 1.0044\n",
      "Epoch 3 Batch 700 Loss 0.7683\n",
      "Epoch 3 Batch 800 Loss 1.2139\n",
      "Epoch 3 Batch 900 Loss 0.7788\n",
      "Epoch 3 Batch 1000 Loss 0.9707\n",
      "Epoch 3 Batch 1100 Loss 0.7395\n",
      "Epoch 3 Batch 1200 Loss 0.6940\n",
      "Epoch 3 Batch 1300 Loss 1.5930\n",
      "Epoch 3 Batch 1400 Loss 1.7599\n",
      "Epoch 3 Batch 1500 Loss 0.6978\n",
      "Epoch 3 Batch 1600 Loss 1.6907\n",
      "Epoch 3 Batch 1700 Loss 0.7966\n",
      "Epoch 3 Batch 1800 Loss 0.8819\n",
      "Epoch 3 Batch 1900 Loss 1.7770\n",
      "Epoch 3 Batch 2000 Loss 0.9685\n",
      "Epoch 3 Batch 2100 Loss 1.0591\n",
      "Epoch 3 Batch 2200 Loss 0.8314\n",
      "Epoch 3 Batch 2300 Loss 0.9484\n",
      "Epoch 3 Batch 2400 Loss 0.8359\n",
      "Epoch 3 Batch 2500 Loss 0.7347\n",
      "Epoch 3 Batch 2600 Loss 0.7633\n",
      "Epoch 3 Batch 2700 Loss 0.8116\n",
      "Epoch 3 Batch 2800 Loss 0.9133\n",
      "Epoch 3 Batch 2900 Loss 1.1676\n",
      "Epoch 3 Batch 3000 Loss 0.7149\n",
      "Epoch 3 Batch 3100 Loss 0.7317\n",
      "Epoch 3 Batch 3200 Loss 0.5853\n",
      "Epoch 3 Batch 3300 Loss 1.5989\n",
      "Epoch 3 Batch 3400 Loss 0.9039\n",
      "Epoch 3 Batch 3500 Loss 0.7333\n",
      "Epoch 3 Batch 3600 Loss 0.8714\n",
      "Epoch 3 Batch 3700 Loss 0.7871\n",
      "Epoch 3 Batch 3800 Loss 0.6108\n",
      "Epoch 3 Batch 3900 Loss 0.5332\n",
      "Epoch 3 Batch 4000 Loss 0.9120\n",
      "Epoch 3 Batch 4100 Loss 0.7099\n",
      "Epoch 3 Batch 4200 Loss 1.2706\n",
      "Epoch 3 Batch 4300 Loss 0.8895\n",
      "Epoch 3 Batch 4400 Loss 1.4149\n",
      "Epoch 3 Batch 4500 Loss 1.3381\n",
      "Epoch 3 Batch 4600 Loss 0.6368\n",
      "Epoch 3 Batch 4700 Loss 0.5380\n",
      "Epoch 3 Batch 4800 Loss 0.5374\n",
      "Epoch 3 Batch 4900 Loss 0.6044\n",
      "Epoch 3 Batch 5000 Loss 0.8132\n",
      "Epoch 3 Batch 5100 Loss 0.7417\n",
      "Epoch 3 Batch 5200 Loss 0.7571\n",
      "Epoch 3 Batch 5300 Loss 1.6095\n",
      "Epoch 3 Batch 5400 Loss 2.5593\n",
      "Epoch 3 Batch 5500 Loss 1.1970\n",
      "Epoch 3 Batch 5600 Loss 0.9480\n",
      "Epoch 3 Batch 5700 Loss 0.7373\n",
      "Epoch 3 Batch 5800 Loss 0.7445\n",
      "Epoch 3 Batch 5900 Loss 1.2554\n",
      "Epoch 3 Batch 6000 Loss 1.3823\n",
      "Epoch 3 Batch 6100 Loss 0.8851\n",
      "Epoch 3 Batch 6200 Loss 0.5228\n",
      "Epoch 3 Batch 6300 Loss 1.0739\n",
      "Epoch 3 Batch 6400 Loss 1.2496\n",
      "Epoch 3 Batch 6500 Loss 1.7260\n",
      "Epoch 3 Batch 6600 Loss 1.1157\n",
      "Epoch 3 Batch 6700 Loss 1.0200\n",
      "Epoch 3 Batch 6800 Loss 0.4307\n",
      "Epoch 3 Batch 6900 Loss 1.1074\n",
      "Epoch 3 Batch 7000 Loss 1.3232\n",
      "Epoch 3 Batch 7100 Loss 2.3172\n",
      "Epoch 3 Batch 7200 Loss 0.7171\n",
      "Epoch 3 Batch 7300 Loss 0.7814\n",
      "Epoch 3 Batch 7400 Loss 1.1204\n",
      "Epoch 3 Batch 7500 Loss 1.7840\n",
      "Epoch 3 Batch 7600 Loss 1.3538\n",
      "Epoch 3 Batch 7700 Loss 0.5501\n",
      "Epoch 3 Batch 7800 Loss 0.9953\n",
      "Epoch 3 Batch 7900 Loss 1.2376\n",
      "Epoch 3 Loss 1.0724\n",
      "Time taken for 1 epoch 803.99689245224 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.8219\n",
      "Epoch 4 Batch 100 Loss 0.8423\n",
      "Epoch 4 Batch 200 Loss 0.9465\n",
      "Epoch 4 Batch 300 Loss 0.7079\n",
      "Epoch 4 Batch 400 Loss 0.3281\n",
      "Epoch 4 Batch 500 Loss 0.4510\n",
      "Epoch 4 Batch 600 Loss 0.6681\n",
      "Epoch 4 Batch 700 Loss 0.6764\n",
      "Epoch 4 Batch 800 Loss 0.9554\n",
      "Epoch 4 Batch 900 Loss 0.8723\n",
      "Epoch 4 Batch 1000 Loss 1.2231\n",
      "Epoch 4 Batch 1100 Loss 0.6119\n",
      "Epoch 4 Batch 1200 Loss 0.7244\n",
      "Epoch 4 Batch 1300 Loss 0.8371\n",
      "Epoch 4 Batch 1400 Loss 1.8540\n",
      "Epoch 4 Batch 1500 Loss 1.7956\n",
      "Epoch 4 Batch 1600 Loss 1.7386\n",
      "Epoch 4 Batch 1700 Loss 0.9054\n",
      "Epoch 4 Batch 1800 Loss 0.7182\n",
      "Epoch 4 Batch 1900 Loss 1.0455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 2000 Loss 1.1007\n",
      "Epoch 4 Batch 2100 Loss 0.8987\n",
      "Epoch 4 Batch 2200 Loss 1.6108\n",
      "Epoch 4 Batch 2300 Loss 0.7839\n",
      "Epoch 4 Batch 2400 Loss 2.0212\n",
      "Epoch 4 Batch 2500 Loss 1.3703\n",
      "Epoch 4 Batch 2600 Loss 1.1384\n",
      "Epoch 4 Batch 2700 Loss 0.1477\n",
      "Epoch 4 Batch 2800 Loss 0.7981\n",
      "Epoch 4 Batch 2900 Loss 0.8430\n",
      "Epoch 4 Batch 3000 Loss 0.6251\n",
      "Epoch 4 Batch 3100 Loss 1.1086\n",
      "Epoch 4 Batch 3200 Loss 0.9761\n",
      "Epoch 4 Batch 3300 Loss 1.0589\n",
      "Epoch 4 Batch 3400 Loss 0.6988\n",
      "Epoch 4 Batch 3500 Loss 1.1598\n",
      "Epoch 4 Batch 3600 Loss 2.4260\n",
      "Epoch 4 Batch 3700 Loss 0.9166\n",
      "Epoch 4 Batch 3800 Loss 1.0400\n",
      "Epoch 4 Batch 3900 Loss 1.4385\n",
      "Epoch 4 Batch 4000 Loss 1.3419\n",
      "Epoch 4 Batch 4100 Loss 0.6452\n",
      "Epoch 4 Batch 4200 Loss 0.5672\n",
      "Epoch 4 Batch 4300 Loss 1.0752\n",
      "Epoch 4 Batch 4400 Loss 0.8034\n",
      "Epoch 4 Batch 4500 Loss 0.7713\n",
      "Epoch 4 Batch 4600 Loss 1.8893\n",
      "Epoch 4 Batch 4700 Loss 1.9108\n",
      "Epoch 4 Batch 4800 Loss 1.5869\n",
      "Epoch 4 Batch 4900 Loss 0.8629\n",
      "Epoch 4 Batch 5000 Loss 1.0742\n",
      "Epoch 4 Batch 5100 Loss 1.6411\n",
      "Epoch 4 Batch 5200 Loss 0.6322\n",
      "Epoch 4 Batch 5300 Loss 0.9645\n",
      "Epoch 4 Batch 5400 Loss 0.2769\n",
      "Epoch 4 Batch 5500 Loss 1.2672\n",
      "Epoch 4 Batch 5600 Loss 0.6074\n",
      "Epoch 4 Batch 5700 Loss 0.7648\n",
      "Epoch 4 Batch 5800 Loss 2.2007\n",
      "Epoch 4 Batch 5900 Loss 0.9782\n",
      "Epoch 4 Batch 6000 Loss 0.7312\n",
      "Epoch 4 Batch 6100 Loss 1.7567\n",
      "Epoch 4 Batch 6200 Loss 0.7874\n",
      "Epoch 4 Batch 6300 Loss 1.0155\n",
      "Epoch 4 Batch 6400 Loss 0.6189\n",
      "Epoch 4 Batch 6500 Loss 0.5716\n",
      "Epoch 4 Batch 6600 Loss 1.2318\n",
      "Epoch 4 Batch 6700 Loss 1.9856\n",
      "Epoch 4 Batch 6800 Loss 0.6285\n",
      "Epoch 4 Batch 6900 Loss 0.5479\n",
      "Epoch 4 Batch 7000 Loss 1.6623\n",
      "Epoch 4 Batch 7100 Loss 1.3539\n",
      "Epoch 4 Batch 7200 Loss 0.6755\n",
      "Epoch 4 Batch 7300 Loss 1.2814\n",
      "Epoch 4 Batch 7400 Loss 0.5589\n",
      "Epoch 4 Batch 7500 Loss 1.3038\n",
      "Epoch 4 Batch 7600 Loss 0.7595\n",
      "Epoch 4 Batch 7700 Loss 0.7688\n",
      "Epoch 4 Batch 7800 Loss 1.1597\n",
      "Epoch 4 Batch 7900 Loss 1.3588\n",
      "Epoch 4 Loss 1.0123\n",
      "Time taken for 1 epoch 752.5765006542206 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9774\n",
      "Epoch 5 Batch 100 Loss 0.4694\n",
      "Epoch 5 Batch 200 Loss 1.1258\n",
      "Epoch 5 Batch 300 Loss 0.5909\n",
      "Epoch 5 Batch 400 Loss 0.6667\n",
      "Epoch 5 Batch 500 Loss 0.5019\n",
      "Epoch 5 Batch 600 Loss 0.9846\n",
      "Epoch 5 Batch 700 Loss 1.1426\n",
      "Epoch 5 Batch 800 Loss 0.8467\n",
      "Epoch 5 Batch 900 Loss 0.7545\n",
      "Epoch 5 Batch 1000 Loss 0.9100\n",
      "Epoch 5 Batch 1100 Loss 0.7870\n",
      "Epoch 5 Batch 1200 Loss 0.8669\n",
      "Epoch 5 Batch 1300 Loss 0.6437\n",
      "Epoch 5 Batch 1400 Loss 0.7053\n",
      "Epoch 5 Batch 1500 Loss 1.4068\n",
      "Epoch 5 Batch 1600 Loss 0.5948\n",
      "Epoch 5 Batch 1700 Loss 0.5094\n",
      "Epoch 5 Batch 1800 Loss 1.1175\n",
      "Epoch 5 Batch 1900 Loss 0.7819\n",
      "Epoch 5 Batch 2000 Loss 1.4901\n",
      "Epoch 5 Batch 2100 Loss 1.4161\n",
      "Epoch 5 Batch 2200 Loss 0.7085\n",
      "Epoch 5 Batch 2300 Loss 0.9555\n",
      "Epoch 5 Batch 2400 Loss 0.4604\n",
      "Epoch 5 Batch 2500 Loss 0.5942\n",
      "Epoch 5 Batch 2600 Loss 0.6809\n",
      "Epoch 5 Batch 2700 Loss 0.8130\n",
      "Epoch 5 Batch 2800 Loss 0.8236\n",
      "Epoch 5 Batch 2900 Loss 0.7359\n",
      "Epoch 5 Batch 3000 Loss 1.0205\n",
      "Epoch 5 Batch 3100 Loss 0.9384\n",
      "Epoch 5 Batch 3200 Loss 0.7049\n",
      "Epoch 5 Batch 3300 Loss 1.1747\n",
      "Epoch 5 Batch 3400 Loss 0.7883\n",
      "Epoch 5 Batch 3500 Loss 1.2880\n",
      "Epoch 5 Batch 3600 Loss 1.2783\n",
      "Epoch 5 Batch 3700 Loss 1.2608\n",
      "Epoch 5 Batch 3800 Loss 0.4517\n",
      "Epoch 5 Batch 3900 Loss 0.8004\n",
      "Epoch 5 Batch 4000 Loss 0.8590\n",
      "Epoch 5 Batch 4100 Loss 0.7561\n",
      "Epoch 5 Batch 4200 Loss 0.8611\n",
      "Epoch 5 Batch 4300 Loss 1.0761\n",
      "Epoch 5 Batch 4400 Loss 0.5664\n",
      "Epoch 5 Batch 4500 Loss 0.8802\n",
      "Epoch 5 Batch 4600 Loss 1.2228\n",
      "Epoch 5 Batch 4700 Loss 1.0376\n",
      "Epoch 5 Batch 4800 Loss 0.3304\n",
      "Epoch 5 Batch 4900 Loss 0.8867\n",
      "Epoch 5 Batch 5000 Loss 0.3546\n",
      "Epoch 5 Batch 5100 Loss 1.2184\n",
      "Epoch 5 Batch 5200 Loss 1.4340\n",
      "Epoch 5 Batch 5300 Loss 0.5719\n",
      "Epoch 5 Batch 5400 Loss 0.5619\n",
      "Epoch 5 Batch 5500 Loss 0.8641\n",
      "Epoch 5 Batch 5600 Loss 0.8922\n",
      "Epoch 5 Batch 5700 Loss 0.3549\n",
      "Epoch 5 Batch 5800 Loss 1.1462\n",
      "Epoch 5 Batch 5900 Loss 1.1887\n",
      "Epoch 5 Batch 6000 Loss 0.0962\n",
      "Epoch 5 Batch 6100 Loss 0.9959\n",
      "Epoch 5 Batch 6200 Loss 0.7341\n",
      "Epoch 5 Batch 6300 Loss 0.2731\n",
      "Epoch 5 Batch 6400 Loss 0.7671\n",
      "Epoch 5 Batch 6500 Loss 0.5326\n",
      "Epoch 5 Batch 6600 Loss 1.3843\n",
      "Epoch 5 Batch 6700 Loss 0.7300\n",
      "Epoch 5 Batch 6800 Loss 2.0884\n",
      "Epoch 5 Batch 6900 Loss 0.7213\n",
      "Epoch 5 Batch 7000 Loss 1.4671\n",
      "Epoch 5 Batch 7100 Loss 0.8096\n",
      "Epoch 5 Batch 7200 Loss 1.1656\n",
      "Epoch 5 Batch 7300 Loss 1.0563\n",
      "Epoch 5 Batch 7400 Loss 0.6759\n",
      "Epoch 5 Batch 7500 Loss 0.8049\n",
      "Epoch 5 Batch 7600 Loss 0.8691\n",
      "Epoch 5 Batch 7700 Loss 0.3815\n",
      "Epoch 5 Batch 7800 Loss 0.8886\n",
      "Epoch 5 Batch 7900 Loss 2.2039\n",
      "Epoch 5 Loss 0.9594\n",
      "Time taken for 1 epoch 737.4988734722137 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2743\n",
      "Epoch 6 Batch 100 Loss 0.7191\n",
      "Epoch 6 Batch 200 Loss 1.0177\n",
      "Epoch 6 Batch 300 Loss 0.6773\n",
      "Epoch 6 Batch 400 Loss 0.7061\n",
      "Epoch 6 Batch 500 Loss 0.9841\n",
      "Epoch 6 Batch 600 Loss 0.7443\n",
      "Epoch 6 Batch 700 Loss 0.9820\n",
      "Epoch 6 Batch 800 Loss 0.8447\n",
      "Epoch 6 Batch 900 Loss 1.2455\n",
      "Epoch 6 Batch 1000 Loss 0.7182\n",
      "Epoch 6 Batch 1100 Loss 1.1131\n",
      "Epoch 6 Batch 1200 Loss 1.6165\n",
      "Epoch 6 Batch 1300 Loss 0.7526\n",
      "Epoch 6 Batch 1400 Loss 0.9396\n",
      "Epoch 6 Batch 1500 Loss 0.8406\n",
      "Epoch 6 Batch 1600 Loss 1.1458\n",
      "Epoch 6 Batch 1700 Loss 0.4851\n",
      "Epoch 6 Batch 1800 Loss 1.3464\n",
      "Epoch 6 Batch 1900 Loss 0.6987\n",
      "Epoch 6 Batch 2000 Loss 0.9263\n",
      "Epoch 6 Batch 2100 Loss 0.9015\n",
      "Epoch 6 Batch 2200 Loss 0.9493\n",
      "Epoch 6 Batch 2300 Loss 0.2669\n",
      "Epoch 6 Batch 2400 Loss 0.7023\n",
      "Epoch 6 Batch 2500 Loss 0.7939\n",
      "Epoch 6 Batch 2600 Loss 1.3840\n",
      "Epoch 6 Batch 2700 Loss 1.3890\n",
      "Epoch 6 Batch 2800 Loss 1.5959\n",
      "Epoch 6 Batch 2900 Loss 0.5549\n",
      "Epoch 6 Batch 3000 Loss 2.1181\n",
      "Epoch 6 Batch 3100 Loss 1.0056\n",
      "Epoch 6 Batch 3200 Loss 1.1207\n",
      "Epoch 6 Batch 3300 Loss 0.7317\n",
      "Epoch 6 Batch 3400 Loss 0.6430\n",
      "Epoch 6 Batch 3500 Loss 0.9864\n",
      "Epoch 6 Batch 3600 Loss 0.5883\n",
      "Epoch 6 Batch 3700 Loss 1.2071\n",
      "Epoch 6 Batch 3800 Loss 0.5937\n",
      "Epoch 6 Batch 3900 Loss 1.9205\n",
      "Epoch 6 Batch 4000 Loss 0.9516\n",
      "Epoch 6 Batch 4100 Loss 0.7041\n",
      "Epoch 6 Batch 4200 Loss 0.6087\n",
      "Epoch 6 Batch 4300 Loss 1.1576\n",
      "Epoch 6 Batch 4400 Loss 1.3122\n",
      "Epoch 6 Batch 4500 Loss 1.0129\n",
      "Epoch 6 Batch 4600 Loss 0.7934\n",
      "Epoch 6 Batch 4700 Loss 0.7461\n",
      "Epoch 6 Batch 4800 Loss 0.9848\n",
      "Epoch 6 Batch 4900 Loss 0.8104\n",
      "Epoch 6 Batch 5000 Loss 0.8951\n",
      "Epoch 6 Batch 5100 Loss 0.9062\n",
      "Epoch 6 Batch 5200 Loss 0.5904\n",
      "Epoch 6 Batch 5300 Loss 1.5240\n",
      "Epoch 6 Batch 5400 Loss 0.9434\n",
      "Epoch 6 Batch 5500 Loss 0.9534\n",
      "Epoch 6 Batch 5600 Loss 0.7948\n",
      "Epoch 6 Batch 5700 Loss 0.3453\n",
      "Epoch 6 Batch 5800 Loss 1.0248\n",
      "Epoch 6 Batch 5900 Loss 1.2276\n",
      "Epoch 6 Batch 6000 Loss 0.7342\n",
      "Epoch 6 Batch 6100 Loss 0.5911\n",
      "Epoch 6 Batch 6200 Loss 0.6044\n",
      "Epoch 6 Batch 6300 Loss 1.0697\n",
      "Epoch 6 Batch 6400 Loss 1.1753\n",
      "Epoch 6 Batch 6500 Loss 0.8613\n",
      "Epoch 6 Batch 6600 Loss 0.7730\n",
      "Epoch 6 Batch 6700 Loss 2.8268\n",
      "Epoch 6 Batch 6800 Loss 1.0457\n",
      "Epoch 6 Batch 6900 Loss 2.8924\n",
      "Epoch 6 Batch 7000 Loss 1.1844\n",
      "Epoch 6 Batch 7100 Loss 0.7389\n",
      "Epoch 6 Batch 7200 Loss 1.4774\n",
      "Epoch 6 Batch 7300 Loss 1.3426\n",
      "Epoch 6 Batch 7400 Loss 1.6097\n",
      "Epoch 6 Batch 7500 Loss 1.2058\n",
      "Epoch 6 Batch 7600 Loss 1.2962\n",
      "Epoch 6 Batch 7700 Loss 1.8890\n",
      "Epoch 6 Batch 7800 Loss 0.6143\n",
      "Epoch 6 Batch 7900 Loss 0.9973\n",
      "Epoch 6 Loss 0.9137\n",
      "Time taken for 1 epoch 737.0198044776917 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8462\n",
      "Epoch 7 Batch 100 Loss 0.7404\n",
      "Epoch 7 Batch 200 Loss 0.4965\n",
      "Epoch 7 Batch 300 Loss 1.0063\n",
      "Epoch 7 Batch 400 Loss 0.8436\n",
      "Epoch 7 Batch 500 Loss 0.5833\n",
      "Epoch 7 Batch 600 Loss 0.7560\n",
      "Epoch 7 Batch 700 Loss 0.7515\n",
      "Epoch 7 Batch 800 Loss 0.6347\n",
      "Epoch 7 Batch 900 Loss 0.9744\n",
      "Epoch 7 Batch 1000 Loss 0.5523\n",
      "Epoch 7 Batch 1100 Loss 1.3495\n",
      "Epoch 7 Batch 1200 Loss 1.6121\n",
      "Epoch 7 Batch 1300 Loss 1.5652\n",
      "Epoch 7 Batch 1400 Loss 0.9450\n",
      "Epoch 7 Batch 1500 Loss 1.2215\n",
      "Epoch 7 Batch 1600 Loss 0.5494\n",
      "Epoch 7 Batch 1700 Loss 0.5716\n",
      "Epoch 7 Batch 1800 Loss 0.4235\n",
      "Epoch 7 Batch 1900 Loss 0.6935\n",
      "Epoch 7 Batch 2000 Loss 0.8462\n",
      "Epoch 7 Batch 2100 Loss 1.0846\n",
      "Epoch 7 Batch 2200 Loss 0.4044\n",
      "Epoch 7 Batch 2300 Loss 1.2635\n",
      "Epoch 7 Batch 2400 Loss 0.6226\n",
      "Epoch 7 Batch 2500 Loss 0.9573\n",
      "Epoch 7 Batch 2600 Loss 0.3602\n",
      "Epoch 7 Batch 2700 Loss 1.1311\n",
      "Epoch 7 Batch 2800 Loss 1.2598\n",
      "Epoch 7 Batch 2900 Loss 1.1305\n",
      "Epoch 7 Batch 3000 Loss 0.7594\n",
      "Epoch 7 Batch 3100 Loss 0.8250\n",
      "Epoch 7 Batch 3200 Loss 0.4736\n",
      "Epoch 7 Batch 3300 Loss 0.6948\n",
      "Epoch 7 Batch 3400 Loss 0.7275\n",
      "Epoch 7 Batch 3500 Loss 0.8614\n",
      "Epoch 7 Batch 3600 Loss 0.9552\n",
      "Epoch 7 Batch 3700 Loss 1.2781\n",
      "Epoch 7 Batch 3800 Loss 0.8131\n",
      "Epoch 7 Batch 3900 Loss 0.2652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 4000 Loss 0.9591\n",
      "Epoch 7 Batch 4100 Loss 2.0651\n",
      "Epoch 7 Batch 4200 Loss 1.4897\n",
      "Epoch 7 Batch 4300 Loss 0.8756\n",
      "Epoch 7 Batch 4400 Loss 0.6262\n",
      "Epoch 7 Batch 4500 Loss 1.3207\n",
      "Epoch 7 Batch 4600 Loss 0.9601\n",
      "Epoch 7 Batch 4700 Loss 0.5284\n",
      "Epoch 7 Batch 4800 Loss 0.7575\n",
      "Epoch 7 Batch 4900 Loss 0.6236\n",
      "Epoch 7 Batch 5000 Loss 1.4375\n",
      "Epoch 7 Batch 5100 Loss 0.9809\n",
      "Epoch 7 Batch 5200 Loss 1.8400\n",
      "Epoch 7 Batch 5300 Loss 0.7477\n",
      "Epoch 7 Batch 5400 Loss 0.7776\n",
      "Epoch 7 Batch 5500 Loss 0.5338\n",
      "Epoch 7 Batch 5600 Loss 0.6591\n",
      "Epoch 7 Batch 5700 Loss 0.4577\n",
      "Epoch 7 Batch 5800 Loss 0.6964\n",
      "Epoch 7 Batch 5900 Loss 0.9755\n",
      "Epoch 7 Batch 6000 Loss 1.1197\n",
      "Epoch 7 Batch 6100 Loss 0.3528\n",
      "Epoch 7 Batch 6200 Loss 0.2153\n",
      "Epoch 7 Batch 6300 Loss 1.3685\n",
      "Epoch 7 Batch 6400 Loss 1.0135\n",
      "Epoch 7 Batch 6500 Loss 0.9165\n",
      "Epoch 7 Batch 6600 Loss 0.8203\n",
      "Epoch 7 Batch 6700 Loss 0.5820\n",
      "Epoch 7 Batch 6800 Loss 0.6745\n",
      "Epoch 7 Batch 6900 Loss 1.3733\n",
      "Epoch 7 Batch 7000 Loss 0.6783\n",
      "Epoch 7 Batch 7100 Loss 2.5899\n",
      "Epoch 7 Batch 7200 Loss 1.8228\n",
      "Epoch 7 Batch 7300 Loss 1.0915\n",
      "Epoch 7 Batch 7400 Loss 0.8345\n",
      "Epoch 7 Batch 7500 Loss 1.5166\n",
      "Epoch 7 Batch 7600 Loss 1.3125\n",
      "Epoch 7 Batch 7700 Loss 0.4218\n",
      "Epoch 7 Batch 7800 Loss 1.8866\n",
      "Epoch 7 Batch 7900 Loss 0.7554\n",
      "Epoch 7 Loss 0.8721\n",
      "Time taken for 1 epoch 777.6181514263153 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7107\n",
      "Epoch 8 Batch 100 Loss 0.4532\n",
      "Epoch 8 Batch 200 Loss 0.8050\n",
      "Epoch 8 Batch 300 Loss 0.4082\n",
      "Epoch 8 Batch 400 Loss 0.6640\n",
      "Epoch 8 Batch 500 Loss 0.4087\n",
      "Epoch 8 Batch 600 Loss 1.0030\n",
      "Epoch 8 Batch 700 Loss 0.8283\n",
      "Epoch 8 Batch 800 Loss 0.5606\n",
      "Epoch 8 Batch 900 Loss 1.5895\n",
      "Epoch 8 Batch 1000 Loss 0.7446\n",
      "Epoch 8 Batch 1100 Loss 0.4784\n",
      "Epoch 8 Batch 1200 Loss 0.6593\n",
      "Epoch 8 Batch 1300 Loss 1.0211\n",
      "Epoch 8 Batch 1400 Loss 0.5291\n",
      "Epoch 8 Batch 1500 Loss 1.0844\n",
      "Epoch 8 Batch 1600 Loss 0.6488\n",
      "Epoch 8 Batch 1700 Loss 0.1337\n",
      "Epoch 8 Batch 1800 Loss 1.1556\n",
      "Epoch 8 Batch 1900 Loss 0.4978\n",
      "Epoch 8 Batch 2000 Loss 0.8240\n",
      "Epoch 8 Batch 2100 Loss 1.0076\n",
      "Epoch 8 Batch 2200 Loss 0.9893\n",
      "Epoch 8 Batch 2300 Loss 1.4000\n",
      "Epoch 8 Batch 2400 Loss 0.6502\n",
      "Epoch 8 Batch 2500 Loss 0.9161\n",
      "Epoch 8 Batch 2600 Loss 0.8619\n",
      "Epoch 8 Batch 2700 Loss 1.1665\n",
      "Epoch 8 Batch 2800 Loss 0.9770\n",
      "Epoch 8 Batch 2900 Loss 1.7526\n",
      "Epoch 8 Batch 3000 Loss 0.7872\n",
      "Epoch 8 Batch 3100 Loss 0.8206\n",
      "Epoch 8 Batch 3200 Loss 1.0144\n",
      "Epoch 8 Batch 3300 Loss 0.5721\n",
      "Epoch 8 Batch 3400 Loss 0.1208\n",
      "Epoch 8 Batch 3500 Loss 1.0414\n",
      "Epoch 8 Batch 3600 Loss 1.7176\n",
      "Epoch 8 Batch 3700 Loss 0.4272\n",
      "Epoch 8 Batch 3800 Loss 0.4498\n",
      "Epoch 8 Batch 3900 Loss 0.3133\n",
      "Epoch 8 Batch 4000 Loss 0.4144\n",
      "Epoch 8 Batch 4100 Loss 0.8315\n",
      "Epoch 8 Batch 4200 Loss 0.7654\n",
      "Epoch 8 Batch 4300 Loss 1.0272\n",
      "Epoch 8 Batch 4400 Loss 0.7431\n",
      "Epoch 8 Batch 4500 Loss 0.8488\n",
      "Epoch 8 Batch 4600 Loss 0.8657\n",
      "Epoch 8 Batch 4700 Loss 0.9237\n",
      "Epoch 8 Batch 4800 Loss 1.2306\n",
      "Epoch 8 Batch 4900 Loss 0.7605\n",
      "Epoch 8 Batch 5000 Loss 1.4646\n",
      "Epoch 8 Batch 5100 Loss 0.7262\n",
      "Epoch 8 Batch 5200 Loss 0.5731\n",
      "Epoch 8 Batch 5300 Loss 0.5430\n",
      "Epoch 8 Batch 5400 Loss 0.5646\n",
      "Epoch 8 Batch 5500 Loss 0.2246\n",
      "Epoch 8 Batch 5600 Loss 0.6857\n",
      "Epoch 8 Batch 5700 Loss 0.4612\n",
      "Epoch 8 Batch 5800 Loss 0.7732\n",
      "Epoch 8 Batch 5900 Loss 0.7846\n",
      "Epoch 8 Batch 6000 Loss 0.8914\n",
      "Epoch 8 Batch 6100 Loss 0.6802\n",
      "Epoch 8 Batch 6200 Loss 0.3475\n",
      "Epoch 8 Batch 6300 Loss 0.8135\n",
      "Epoch 8 Batch 6400 Loss 0.6722\n",
      "Epoch 8 Batch 6500 Loss 0.4128\n",
      "Epoch 8 Batch 6600 Loss 0.4844\n",
      "Epoch 8 Batch 6700 Loss 1.3216\n",
      "Epoch 8 Batch 6800 Loss 0.6924\n",
      "Epoch 8 Batch 6900 Loss 1.8098\n",
      "Epoch 8 Batch 7000 Loss 1.3219\n",
      "Epoch 8 Batch 7100 Loss 0.8853\n",
      "Epoch 8 Batch 7200 Loss 1.0220\n",
      "Epoch 8 Batch 7300 Loss 0.5930\n",
      "Epoch 8 Batch 7400 Loss 1.0766\n",
      "Epoch 8 Batch 7500 Loss 0.2093\n",
      "Epoch 8 Batch 7600 Loss 0.4454\n",
      "Epoch 8 Batch 7700 Loss 0.5073\n",
      "Epoch 8 Batch 7800 Loss 1.4026\n",
      "Epoch 8 Batch 7900 Loss 0.8052\n",
      "Epoch 8 Loss 0.8357\n",
      "Time taken for 1 epoch 791.1662163734436 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7138\n",
      "Epoch 9 Batch 100 Loss 0.4757\n",
      "Epoch 9 Batch 200 Loss 0.4242\n",
      "Epoch 9 Batch 300 Loss 0.6705\n",
      "Epoch 9 Batch 400 Loss 0.6954\n",
      "Epoch 9 Batch 500 Loss 1.1684\n",
      "Epoch 9 Batch 600 Loss 0.4512\n",
      "Epoch 9 Batch 700 Loss 0.5159\n",
      "Epoch 9 Batch 800 Loss 0.7113\n",
      "Epoch 9 Batch 900 Loss 0.8855\n",
      "Epoch 9 Batch 1000 Loss 0.9022\n",
      "Epoch 9 Batch 1100 Loss 0.6919\n",
      "Epoch 9 Batch 1200 Loss 0.8951\n",
      "Epoch 9 Batch 1300 Loss 0.6214\n",
      "Epoch 9 Batch 1400 Loss 0.8569\n",
      "Epoch 9 Batch 1500 Loss 0.2547\n",
      "Epoch 9 Batch 1600 Loss 0.6192\n",
      "Epoch 9 Batch 1700 Loss 0.5631\n",
      "Epoch 9 Batch 1800 Loss 0.6472\n",
      "Epoch 9 Batch 1900 Loss 0.1692\n",
      "Epoch 9 Batch 2000 Loss 0.8710\n",
      "Epoch 9 Batch 2100 Loss 0.3915\n",
      "Epoch 9 Batch 2200 Loss 0.2692\n",
      "Epoch 9 Batch 2300 Loss 0.9406\n",
      "Epoch 9 Batch 2400 Loss 1.1270\n",
      "Epoch 9 Batch 2500 Loss 0.8895\n",
      "Epoch 9 Batch 2600 Loss 0.9627\n",
      "Epoch 9 Batch 2700 Loss 1.2400\n",
      "Epoch 9 Batch 2800 Loss 0.8895\n",
      "Epoch 9 Batch 2900 Loss 1.6833\n",
      "Epoch 9 Batch 3000 Loss 1.5743\n",
      "Epoch 9 Batch 3100 Loss 1.7136\n",
      "Epoch 9 Batch 3200 Loss 0.5585\n",
      "Epoch 9 Batch 3300 Loss 0.3756\n",
      "Epoch 9 Batch 3400 Loss 0.6936\n",
      "Epoch 9 Batch 3500 Loss 0.6128\n",
      "Epoch 9 Batch 3600 Loss 0.3376\n",
      "Epoch 9 Batch 3700 Loss 0.5017\n",
      "Epoch 9 Batch 3800 Loss 0.9093\n",
      "Epoch 9 Batch 3900 Loss 2.5034\n",
      "Epoch 9 Batch 4000 Loss 1.7020\n",
      "Epoch 9 Batch 4100 Loss 0.9532\n",
      "Epoch 9 Batch 4200 Loss 0.4575\n",
      "Epoch 9 Batch 4300 Loss 0.6976\n",
      "Epoch 9 Batch 4400 Loss 0.6664\n",
      "Epoch 9 Batch 4500 Loss 1.2294\n",
      "Epoch 9 Batch 4600 Loss 0.7127\n",
      "Epoch 9 Batch 4700 Loss 0.8995\n",
      "Epoch 9 Batch 4800 Loss 1.0992\n",
      "Epoch 9 Batch 4900 Loss 2.2811\n",
      "Epoch 9 Batch 5000 Loss 0.8117\n",
      "Epoch 9 Batch 5100 Loss 0.5390\n",
      "Epoch 9 Batch 5200 Loss 0.9625\n",
      "Epoch 9 Batch 5300 Loss 0.4702\n",
      "Epoch 9 Batch 5400 Loss 0.5647\n",
      "Epoch 9 Batch 5500 Loss 0.7942\n",
      "Epoch 9 Batch 5600 Loss 0.3761\n",
      "Epoch 9 Batch 5700 Loss 0.6980\n",
      "Epoch 9 Batch 5800 Loss 0.4113\n",
      "Epoch 9 Batch 5900 Loss 0.5645\n",
      "Epoch 9 Batch 6000 Loss 0.5149\n",
      "Epoch 9 Batch 6100 Loss 0.9728\n",
      "Epoch 9 Batch 6200 Loss 1.1559\n",
      "Epoch 9 Batch 6300 Loss 0.7937\n",
      "Epoch 9 Batch 6400 Loss 3.3946\n",
      "Epoch 9 Batch 6500 Loss 1.5328\n",
      "Epoch 9 Batch 6600 Loss 0.9733\n",
      "Epoch 9 Batch 6700 Loss 0.5979\n",
      "Epoch 9 Batch 6800 Loss 0.3468\n",
      "Epoch 9 Batch 6900 Loss 0.8587\n",
      "Epoch 9 Batch 7000 Loss 1.4257\n",
      "Epoch 9 Batch 7100 Loss 1.4415\n",
      "Epoch 9 Batch 7200 Loss 0.7711\n",
      "Epoch 9 Batch 7300 Loss 1.2906\n",
      "Epoch 9 Batch 7400 Loss 1.1140\n",
      "Epoch 9 Batch 7500 Loss 0.4802\n",
      "Epoch 9 Batch 7600 Loss 0.5770\n",
      "Epoch 9 Batch 7700 Loss 1.9808\n",
      "Epoch 9 Batch 7800 Loss 0.9983\n",
      "Epoch 9 Batch 7900 Loss 0.7180\n",
      "Epoch 9 Loss 0.8015\n",
      "Time taken for 1 epoch 769.481546163559 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6690\n",
      "Epoch 10 Batch 100 Loss 0.1667\n",
      "Epoch 10 Batch 200 Loss 0.8292\n",
      "Epoch 10 Batch 300 Loss 0.5385\n",
      "Epoch 10 Batch 400 Loss 0.2678\n",
      "Epoch 10 Batch 500 Loss 0.8599\n",
      "Epoch 10 Batch 600 Loss 0.5089\n",
      "Epoch 10 Batch 700 Loss 0.3472\n",
      "Epoch 10 Batch 800 Loss 0.6601\n",
      "Epoch 10 Batch 900 Loss 0.7924\n",
      "Epoch 10 Batch 1000 Loss 0.3847\n",
      "Epoch 10 Batch 1100 Loss 0.7986\n",
      "Epoch 10 Batch 1200 Loss 1.0418\n",
      "Epoch 10 Batch 1300 Loss 0.3851\n",
      "Epoch 10 Batch 1400 Loss 0.7653\n",
      "Epoch 10 Batch 1500 Loss 1.9241\n",
      "Epoch 10 Batch 1600 Loss 0.4491\n",
      "Epoch 10 Batch 1700 Loss 0.3311\n",
      "Epoch 10 Batch 1800 Loss 0.5544\n",
      "Epoch 10 Batch 1900 Loss 0.7728\n",
      "Epoch 10 Batch 2000 Loss 0.5563\n",
      "Epoch 10 Batch 2100 Loss 1.3222\n",
      "Epoch 10 Batch 2200 Loss 0.8556\n",
      "Epoch 10 Batch 2300 Loss 0.2553\n",
      "Epoch 10 Batch 2400 Loss 1.2542\n",
      "Epoch 10 Batch 2500 Loss 0.6733\n",
      "Epoch 10 Batch 2600 Loss 0.7285\n",
      "Epoch 10 Batch 2700 Loss 0.5343\n",
      "Epoch 10 Batch 2800 Loss 0.5769\n",
      "Epoch 10 Batch 2900 Loss 0.8184\n",
      "Epoch 10 Batch 3000 Loss 1.0208\n",
      "Epoch 10 Batch 3100 Loss 1.0503\n",
      "Epoch 10 Batch 3200 Loss 0.9099\n",
      "Epoch 10 Batch 3300 Loss 1.0903\n",
      "Epoch 10 Batch 3400 Loss 0.7755\n",
      "Epoch 10 Batch 3500 Loss 0.9635\n",
      "Epoch 10 Batch 3600 Loss 0.8729\n",
      "Epoch 10 Batch 3700 Loss 0.1121\n",
      "Epoch 10 Batch 3800 Loss 0.3310\n",
      "Epoch 10 Batch 3900 Loss 1.0670\n",
      "Epoch 10 Batch 4000 Loss 0.6294\n",
      "Epoch 10 Batch 4100 Loss 0.5598\n",
      "Epoch 10 Batch 4200 Loss 0.5529\n",
      "Epoch 10 Batch 4300 Loss 1.4908\n",
      "Epoch 10 Batch 4400 Loss 1.0879\n",
      "Epoch 10 Batch 4500 Loss 1.6599\n",
      "Epoch 10 Batch 4600 Loss 0.4946\n",
      "Epoch 10 Batch 4700 Loss 1.2331\n",
      "Epoch 10 Batch 4800 Loss 0.9913\n",
      "Epoch 10 Batch 4900 Loss 0.6508\n",
      "Epoch 10 Batch 5000 Loss 0.9294\n",
      "Epoch 10 Batch 5100 Loss 0.4639\n",
      "Epoch 10 Batch 5200 Loss 0.4231\n",
      "Epoch 10 Batch 5300 Loss 0.6481\n",
      "Epoch 10 Batch 5400 Loss 0.6597\n",
      "Epoch 10 Batch 5500 Loss 1.1897\n",
      "Epoch 10 Batch 5600 Loss 0.7574\n",
      "Epoch 10 Batch 5700 Loss 0.3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 5800 Loss 0.5067\n",
      "Epoch 10 Batch 5900 Loss 0.5061\n",
      "Epoch 10 Batch 6000 Loss 0.7088\n",
      "Epoch 10 Batch 6100 Loss 0.7320\n",
      "Epoch 10 Batch 6200 Loss 0.4180\n",
      "Epoch 10 Batch 6300 Loss 0.9419\n",
      "Epoch 10 Batch 6400 Loss 0.3827\n",
      "Epoch 10 Batch 6500 Loss 0.6148\n",
      "Epoch 10 Batch 6600 Loss 0.6951\n",
      "Epoch 10 Batch 6700 Loss 0.8594\n",
      "Epoch 10 Batch 6800 Loss 1.4661\n",
      "Epoch 10 Batch 6900 Loss 0.8018\n",
      "Epoch 10 Batch 7000 Loss 0.3941\n",
      "Epoch 10 Batch 7100 Loss 1.5144\n",
      "Epoch 10 Batch 7200 Loss 1.2956\n",
      "Epoch 10 Batch 7300 Loss 1.2861\n",
      "Epoch 10 Batch 7400 Loss 0.9262\n",
      "Epoch 10 Batch 7500 Loss 0.3403\n",
      "Epoch 10 Batch 7600 Loss 1.3345\n",
      "Epoch 10 Batch 7700 Loss 0.5240\n",
      "Epoch 10 Batch 7800 Loss 0.5863\n",
      "Epoch 10 Batch 7900 Loss 0.2477\n",
      "Epoch 10 Loss 0.7698\n",
      "Time taken for 1 epoch 747.1710212230682 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Translate\n",
    "\n",
    "The evaluate function is similar to the training loop, except we don't use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
    "Stop predicting when the model predicts the end token.\n",
    "And store the attention weights for every time step.\n",
    "\n",
    "Note: The encoder output is calculated only once for one input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weigths to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[0][0].numpy()\n",
    "\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: that s alive . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hoy es miercoles <end>\n",
      "Predicted translation: this is life . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('hoy es miercoles', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
